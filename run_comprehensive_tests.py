#!/usr/bin/env python3
"""
Comprehensive Test Runner for IndexTTS Streaming Performance Analysis
================================================================================
æ¸¬è©¦ç¶­åº¦:
1. é¦–æ¬¡éŸ¿æ‡‰æ™‚é–“ (TTFB - Time To First Byte)
2. ç¸½ç”Ÿæˆæ™‚é–“
3. ç”Ÿæˆå€ç‡ (é€Ÿåº¦)
4. è¨˜æ†¶é«”ä½¿ç”¨
5. éŸ³è³ªç©©å®šæ€§
6. ç¸½è€—æ™‚
7. ä½¿ç”¨è€…æ„ŸçŸ¥å»¶é²
8. è³‡æºä½¿ç”¨æ•ˆç‡

è¼¸å‡º:
- CSV çµ±è¨ˆæ•¸æ“šè¡¨æ ¼
- JSON è©³ç´°æ¸¬è©¦æ—¥èªŒ
- 2 å¼µè¦–è¦ºåŒ–æ¯”è¼ƒåœ–è¡¨ (PNG)
"""

import sys
import os
import subprocess
import json
import csv
import time
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import traceback

# å¯é¸ä¾è³´ - ç”¨æ–¼è¦–è¦ºåŒ–
try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # éäº’å‹•å¼å¾Œç«¯
    import numpy as np
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("âš ï¸  matplotlib æœªå®‰è£ï¼Œå°‡è·³éåœ–è¡¨ç”Ÿæˆ")
    print("   å®‰è£æ–¹å¼: pip install matplotlib")

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    print("âš ï¸  psutil æœªå®‰è£ï¼Œå°‡è·³éè¨˜æ†¶é«”ç›£æ§")
    print("   å®‰è£æ–¹å¼: pip install psutil")

# ==================== é…ç½®å€ ====================

# æ¸¬è©¦æ–‡æœ¬ (çµ±ä¸€ä½¿ç”¨ï¼Œç¢ºä¿å¯æ¯”æ€§)
DEFAULT_TEXT = (
    "åŠ‰ä½©çœŸåˆ†æï¼Œè¡Œæ”¿é™¢ã€Œé–‹æ°´é¾é ­ã€ï¼Œ9æœˆåˆæ–°é’å®‰é¬†ç¶ï¼ŒåŠå»¶é•·å°å…ˆè²·å¾Œè³£æ›å±‹æ—å‡ºå”®èˆŠå±‹çš„æœŸé™ï¼Œ"
    "è§€æœ›çš„å¸‚å ´æ°›åœç¨æ¸›ï¼Œæˆ¿å¸‚äº¤æ˜“é‡å‡ºç¾å°å¹…æˆé•·ï¼Œ"
    "äº‹å¯¦ä¸Šï¼Œä»Šå¹´æˆ¿å¸‚çš„äº¤æ˜“çµæ§‹å·²å¾å»å¹´çš„åƒ¹é‡é½Šæšï¼Œåˆ°ä»Šå¹´çš„é‡ç¸®ã€åƒ¹æ ¼ç·©è·Œã€‚"
    "ç›®å‰æˆ¿åƒ¹çš„è·Œå¹…æ–¹é¢ï¼Œç›¸è¼ƒæ–¼å»å¹´é‚„æœ‰éå¸¸ä½å€‹ä½æ•¸çš„ä¸‹æ»‘ï¼Œé¡¯ç¤ºæˆ¿å¸‚è³£æ–¹å¯¦éš›ä¸Šæ²’æœ‰å‡ºè„«çš„å£“åŠ›ã€‚"
)

# æ¸¬è©¦è…³æœ¬è·¯å¾‘
SCRIPT_DIR = Path(__file__).parent
TEST_SCRIPT = SCRIPT_DIR / "test_streaming_with_output.py"  # ä½¿ç”¨æ”¯æŒè¼¸å‡ºçš„ç‰ˆæœ¬
OUTPUT_DIR = SCRIPT_DIR / "benchmark_output"
AUDIO_OUTPUT_DIR = OUTPUT_DIR / "audio_samples"

# åƒè€ƒéŸ³æª”è·¯å¾‘
INDEX_TTS_DIR = SCRIPT_DIR
REF_AUDIO_DIR = INDEX_TTS_DIR / "examples"

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
OUTPUT_DIR.mkdir(exist_ok=True)
AUDIO_OUTPUT_DIR.mkdir(exist_ok=True)

# ==================== æ¸¬è©¦é…ç½® ====================

# Test Suite 1: Voice Comparison (voice_06 vs voice_07)
TEST_SUITE_1 = [
    {
        "name": "voice_06_baseline",
        "description": "Voice 06 - Baseline (Default Parameters)",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_06.wav"),
            "--text", DEFAULT_TEXT,
            "--warmup"
        ]
    },
    {
        "name": "voice_07_baseline",
        "description": "Voice 07 - Baseline (Default Parameters)",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--warmup"
        ]
    }
]

# Test Suite 2: Speed Strategy Comparison (voice_07)
TEST_SUITE_2 = [
    {
        "name": "voice_07_no_speed",
        "description": "Voice 07 - No Speed Modification",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--warmup",
            "--output", str(AUDIO_OUTPUT_DIR / "voice_07_no_speed.wav")
        ]
    },
    {
        "name": "voice_07_pre_speed_1.2x",
        "description": "Voice 07 - Pre-Speed 1.2x (Reference Audio Acceleration)",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--pre_speed_ref", "1.2",
            "--warmup",
            "--output", str(AUDIO_OUTPUT_DIR / "voice_07_pre_speed_1.2x.wav")
        ]
    },
    {
        "name": "voice_07_post_speed_1.2x",
        "description": "Voice 07 - Post-Speed 1.2x (Playback Acceleration)",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--speed", "1.2",
            "--warmup",
            "--output", str(AUDIO_OUTPUT_DIR / "voice_07_post_speed_1.2x.wav")
        ]
    },
    {
        "name": "voice_07_hybrid_speed_1.2x",
        "description": "Voice 07 - Hybrid Speed 1.2x (Pre + Post)",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--pre_speed_ref", "1.2",
            "--speed", "1.2",
            "--warmup",
            "--output", str(AUDIO_OUTPUT_DIR / "voice_07_hybrid_speed_1.2x.wav")
        ]
    }
]

# Test Suite 3: Version & Mode Comparison (voice_07)
TEST_SUITE_3 = [
    {
        "name": "v1_streaming",
        "description": "V1 - Streaming Mode",
        "args": [
            "--version", "v1",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--warmup"
        ]
    },
    {
        "name": "v2_streaming_token",
        "description": "V2 - Streaming Mode (Token-based)",
        "args": [
            "--version", "v2",
            "--method", "token",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--warmup"
        ]
    },
    {
        "name": "v2_streaming_word",
        "description": "V2 - Streaming Mode (Word-based)",
        "args": [
            "--version", "v2",
            "--method", "word",
            "--ref_audio", str(REF_AUDIO_DIR / "voice_07.wav"),
            "--text", DEFAULT_TEXT,
            "--warmup"
        ]
    }
]

# ==================== è§£æå‡½æ•¸ ====================

def parse_test_output(output: str) -> Dict[str, Any]:
    """
    è§£ææ¸¬è©¦è¼¸å‡ºï¼Œæå–é—œéµæŒ‡æ¨™
    """
    metrics = {
        "ttfb": None,  # Time to first byte (é¦–æ¬¡éŸ¿æ‡‰æ™‚é–“)
        "total_time": None,  # ç¸½è€—æ™‚
        "avg_gen_rate": None,  # å¹³å‡ç”Ÿæˆå€ç‡
        "max_gen_rate": None,  # æœ€å¤§ç”Ÿæˆå€ç‡
        "min_gen_rate": None,  # æœ€å°ç”Ÿæˆå€ç‡
        "overall_rtf": None,  # æ•´é«”å¯¦æ™‚ç‡
        "parallel_efficiency": None,  # ä¸¦è¡Œæ•ˆç‡
        "chunk_count": 0,  # éŸ³è¨Šç‰‡æ®µæ•¸
        "warmup_time": None,  # é ç†±æ™‚é–“
        "model_load_time": None,  # æ¨¡å‹è¼‰å…¥æ™‚é–“
        "ref_audio_duration": None,  # åƒè€ƒéŸ³æª”é•·åº¦
        "ref_audio_size_mb": None,  # åƒè€ƒéŸ³æª”å¤§å°
        "pre_speed_enabled": False,  # æ˜¯å¦ä½¿ç”¨é åŠ é€Ÿ
        "post_speed_enabled": False,  # æ˜¯å¦ä½¿ç”¨å¾Œè™•ç†åŠ é€Ÿ
        "error": None  # éŒ¯èª¤ä¿¡æ¯
    }

    try:
        # TTFB (é¦–æ¬¡éŸ¿æ‡‰)
        ttfb_match = re.search(r'\[âš¡ First Token\].*?(\d+\.\d+)s', output)
        if ttfb_match:
            metrics["ttfb"] = float(ttfb_match.group(1))

        # ç¸½è€—æ™‚
        total_match = re.search(r'ç¸½è€—æ™‚:\s*(\d+\.\d+)\s*s', output)
        if total_match:
            metrics["total_time"] = float(total_match.group(1))

        # ç”Ÿæˆå€ç‡ (Audio/Process Speed)
        avg_rate_match = re.search(r'Avg\s*:\s*(\d+\.\d+)\s*x', output)
        if avg_rate_match:
            metrics["avg_gen_rate"] = float(avg_rate_match.group(1))

        max_rate_match = re.search(r'Max\s*:\s*(\d+\.\d+)\s*x', output)
        if max_rate_match:
            metrics["max_gen_rate"] = float(max_rate_match.group(1))

        min_rate_match = re.search(r'Min\s*:\s*(\d+\.\d+)\s*x', output)
        if min_rate_match:
            metrics["min_gen_rate"] = float(min_rate_match.group(1))

        # æ•´é«”å¯¦æ™‚ç‡ (RTF)
        rtf_match = re.search(r'æ•´é«”å¯¦æ™‚ç‡.*?RTF.*?(\d+\.\d+)', output)
        if rtf_match:
            metrics["overall_rtf"] = float(rtf_match.group(1))

        # ä¸¦è¡Œæ•ˆç‡
        parallel_match = re.search(r'ä¸¦è¡Œæ•ˆç‡.*?(\d+\.\d+)%', output)
        if parallel_match:
            metrics["parallel_efficiency"] = float(parallel_match.group(1))

        # é ç†±æ™‚é–“
        warmup_match = re.search(r'é ç†±å®Œæˆ.*?è€—æ™‚:\s*(\d+\.\d+)s', output)
        if warmup_match:
            metrics["warmup_time"] = float(warmup_match.group(1))

        # æ¨¡å‹è¼‰å…¥æ™‚é–“
        load_match = re.search(r'æ¨¡å‹è¼‰å…¥å®Œæˆ.*?è€—æ™‚:\s*(\d+\.\d+)s', output)
        if load_match:
            metrics["model_load_time"] = float(load_match.group(1))

        # åƒè€ƒéŸ³æª”ä¿¡æ¯
        duration_match = re.search(r'ç§’æ•¸.*?Length.*?:\s*(\d+\.\d+)\s*s', output)
        if duration_match:
            metrics["ref_audio_duration"] = float(duration_match.group(1))

        size_match = re.search(r'å¤§å°.*?Size.*?:\s*(\d+\.\d+)\s*MB', output)
        if size_match:
            metrics["ref_audio_size_mb"] = float(size_match.group(1))

        # æª¢æ¸¬åŠ é€Ÿç­–ç•¥
        if "åƒè€ƒéŸ³æª”åŠ é€Ÿ" in output or "pre_speed_ref" in output:
            metrics["pre_speed_enabled"] = True

        if "æ’­æ”¾å¾Œè£½åŠ é€Ÿ" in output or "--speed" in output:
            metrics["post_speed_enabled"] = True

        # ç‰‡æ®µè¨ˆæ•¸
        chunk_matches = re.findall(r'\[Queue\] ç‰‡æ®µ\s*(\d+)', output)
        if chunk_matches:
            metrics["chunk_count"] = max([int(c) for c in chunk_matches])

    except Exception as e:
        metrics["error"] = f"è§£æéŒ¯èª¤: {str(e)}"

    return metrics


def run_single_test(test_config: Dict[str, Any], test_id: str) -> Dict[str, Any]:
    """
    åŸ·è¡Œå–®ä¸€æ¸¬è©¦ä¸¦æ”¶é›†çµæœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æ¸¬è©¦: {test_config['name']}")
    print(f"   {test_config['description']}")
    print(f"{'='*80}")

    result = {
        "test_id": test_id,
        "name": test_config["name"],
        "description": test_config["description"],
        "timestamp": datetime.now().isoformat(),
        "metrics": {},
        "raw_output": "",
        "error": None,
        "memory_usage_mb": None
    }

    # è¨˜æ†¶é«”ç›£æ§ (å¦‚æœå¯ç”¨)
    initial_memory = None
    if HAS_PSUTIL:
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

    try:
        # åŸ·è¡Œæ¸¬è©¦
        cmd = [sys.executable, str(TEST_SCRIPT)] + test_config["args"]

        print(f"ğŸ“ åŸ·è¡Œå‘½ä»¤:")
        print(f"   {' '.join(cmd)}\n")

        start_time = time.time()

        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10 åˆ†é˜è¶…æ™‚
        )

        execution_time = time.time() - start_time

        # è¨˜éŒ„è¼¸å‡º
        result["raw_output"] = proc.stdout
        result["stderr"] = proc.stderr
        result["execution_time"] = execution_time
        result["return_code"] = proc.returncode

        # è¨˜æ†¶é«”ä½¿ç”¨
        if HAS_PSUTIL and initial_memory:
            final_memory = process.memory_info().rss / 1024 / 1024
            result["memory_usage_mb"] = final_memory - initial_memory

        # è§£ææŒ‡æ¨™
        if proc.returncode == 0:
            result["metrics"] = parse_test_output(proc.stdout)
            print(f"âœ… æ¸¬è©¦å®Œæˆ (è€—æ™‚: {execution_time:.2f}s)")

            # é¡¯ç¤ºé—œéµæŒ‡æ¨™
            m = result["metrics"]
            if m.get("ttfb"):
                print(f"   âš¡ TTFB: {m['ttfb']:.2f}s")
            if m.get("total_time"):
                print(f"   â±ï¸  ç¸½è€—æ™‚: {m['total_time']:.2f}s")
            if m.get("avg_gen_rate"):
                print(f"   ğŸš€ å¹³å‡ç”Ÿæˆå€ç‡: {m['avg_gen_rate']:.2f}x")
            if m.get("overall_rtf"):
                print(f"   ğŸ“Š æ•´é«” RTF: {m['overall_rtf']:.3f}")
        else:
            result["error"] = f"æ¸¬è©¦å¤±æ•— (è¿”å›ç¢¼: {proc.returncode})"
            print(f"âŒ {result['error']}")
            if proc.stderr:
                print(f"   éŒ¯èª¤è¼¸å‡º:\n{proc.stderr}")

    except subprocess.TimeoutExpired:
        result["error"] = "æ¸¬è©¦è¶…æ™‚ (>600s)"
        print(f"âŒ {result['error']}")
    except Exception as e:
        result["error"] = f"åŸ·è¡ŒéŒ¯èª¤: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {result['error']}")

    return result




def save_results_csv(results: List[Dict[str, Any]], output_path: Path):
    """
    ä¿å­˜çµæœç‚º CSV æ ¼å¼
    """
    if not results:
        return

    # æå–æ‰€æœ‰æŒ‡æ¨™éµ
    all_metric_keys = set()
    for r in results:
        if r.get("metrics"):
            all_metric_keys.update(r["metrics"].keys())

    # æ§‹å»º CSV è¡¨é ­
    fieldnames = [
        "test_id", "name", "description", "timestamp",
        "execution_time", "memory_usage_mb", "error"
    ] + sorted(all_metric_keys)

    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for result in results:
            row = {
                "test_id": result.get("test_id"),
                "name": result.get("name"),
                "description": result.get("description"),
                "timestamp": result.get("timestamp"),
                "execution_time": result.get("execution_time"),
                "memory_usage_mb": result.get("memory_usage_mb"),
                "error": result.get("error")
            }

            # æ·»åŠ æŒ‡æ¨™
            if result.get("metrics"):
                for key in all_metric_keys:
                    row[key] = result["metrics"].get(key)

            writer.writerow(row)

    print(f"âœ… CSV å·²ä¿å­˜: {output_path}")


def save_results_json(results: List[Dict[str, Any]], output_path: Path):
    """
    ä¿å­˜å®Œæ•´çµæœç‚º JSON æ ¼å¼
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"âœ… JSON å·²ä¿å­˜: {output_path}")


def generate_visualization_1(results: List[Dict[str, Any]], output_path: Path):
    """
    è¦–è¦ºåŒ– 1: TTFB vs ç¸½è€—æ™‚ vs ç”Ÿæˆå€ç‡ (æŸ±ç‹€åœ–)
    """
    if not HAS_MATPLOTLIB:
        print("âš ï¸  è·³éåœ–è¡¨ç”Ÿæˆ (matplotlib æœªå®‰è£)")
        return

    # æå–æ•¸æ“š
    names = []
    ttfb_values = []
    total_time_values = []
    gen_rate_values = []

    for r in results:
        if r.get("error"):
            continue

        m = r.get("metrics", {})
        names.append(r["name"])
        ttfb_values.append(m.get("ttfb") or 0)
        total_time_values.append(m.get("total_time") or 0)
        gen_rate_values.append(m.get("avg_gen_rate") or 0)

    if not names:
        print("âš ï¸  ç„¡æœ‰æ•ˆæ•¸æ“šï¼Œè·³éåœ–è¡¨ç”Ÿæˆ")
        return

    # å‰µå»ºåœ–è¡¨
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('IndexTTS Performance Comparison', fontsize=16, fontweight='bold')

    x_pos = np.arange(len(names))

    # å­åœ– 1: TTFB (è¶Šä½è¶Šå¥½)
    axes[0].bar(x_pos, ttfb_values, color='steelblue', alpha=0.8)
    axes[0].set_ylabel('Time (seconds)', fontsize=12)
    axes[0].set_title('Time To First Byte (TTFB)', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x_pos)
    axes[0].set_xticklabels(names, rotation=45, ha='right')
    axes[0].grid(axis='y', alpha=0.3)

    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for i, v in enumerate(ttfb_values):
        axes[0].text(i, v + max(ttfb_values)*0.02, f'{v:.2f}s',
                    ha='center', va='bottom', fontsize=9)

    # å­åœ– 2: ç¸½è€—æ™‚ (è¶Šä½è¶Šå¥½)
    axes[1].bar(x_pos, total_time_values, color='coral', alpha=0.8)
    axes[1].set_ylabel('Time (seconds)', fontsize=12)
    axes[1].set_title('Total Generation Time', fontsize=14, fontweight='bold')
    axes[1].set_xticks(x_pos)
    axes[1].set_xticklabels(names, rotation=45, ha='right')
    axes[1].grid(axis='y', alpha=0.3)

    for i, v in enumerate(total_time_values):
        axes[1].text(i, v + max(total_time_values)*0.02, f'{v:.2f}s',
                    ha='center', va='bottom', fontsize=9)

    # å­åœ– 3: ç”Ÿæˆå€ç‡ (è¶Šé«˜è¶Šå¥½)
    axes[2].bar(x_pos, gen_rate_values, color='mediumseagreen', alpha=0.8)
    axes[2].set_ylabel('Rate (x)', fontsize=12)
    axes[2].set_title('Average Generation Rate', fontsize=14, fontweight='bold')
    axes[2].set_xticks(x_pos)
    axes[2].set_xticklabels(names, rotation=45, ha='right')
    axes[2].grid(axis='y', alpha=0.3)
    axes[2].axhline(y=1.0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Real-time (1.0x)')
    axes[2].legend()

    for i, v in enumerate(gen_rate_values):
        axes[2].text(i, v + max(gen_rate_values)*0.02, f'{v:.2f}x',
                    ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ… åœ–è¡¨ 1 å·²ä¿å­˜: {output_path}")


def generate_visualization_2(results: List[Dict[str, Any]], output_path: Path):
    """
    è¦–è¦ºåŒ– 2: RTF vs ä¸¦è¡Œæ•ˆç‡ vs è¨˜æ†¶é«”ä½¿ç”¨ (ç¶œåˆé›·é”åœ–æˆ–å¤šè»¸åœ–)
    """
    if not HAS_MATPLOTLIB:
        print("âš ï¸  è·³éåœ–è¡¨ç”Ÿæˆ (matplotlib æœªå®‰è£)")
        return

    # æå–æ•¸æ“š
    names = []
    rtf_values = []
    parallel_values = []
    memory_values = []

    for r in results:
        if r.get("error"):
            continue

        m = r.get("metrics", {})
        names.append(r["name"])
        rtf_values.append(m.get("overall_rtf") or 0)
        parallel_values.append(m.get("parallel_efficiency") or 0)
        memory_values.append(r.get("memory_usage_mb") or 0)

    if not names:
        print("âš ï¸  ç„¡æœ‰æ•ˆæ•¸æ“šï¼Œè·³éåœ–è¡¨ç”Ÿæˆ")
        return

    # å‰µå»ºåœ–è¡¨
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('IndexTTS Efficiency & Resource Analysis', fontsize=16, fontweight='bold')

    x_pos = np.arange(len(names))

    # å­åœ– 1: RTF (è¶Šä½è¶Šå¥½)
    axes[0].bar(x_pos, rtf_values, color='orchid', alpha=0.8)
    axes[0].set_ylabel('RTF (lower is better)', fontsize=12)
    axes[0].set_title('Overall Real-Time Factor', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x_pos)
    axes[0].set_xticklabels(names, rotation=45, ha='right')
    axes[0].grid(axis='y', alpha=0.3)
    axes[0].axhline(y=1.0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Real-time threshold')
    axes[0].legend()

    for i, v in enumerate(rtf_values):
        axes[0].text(i, v + max(rtf_values)*0.02 if rtf_values else 0.01,
                    f'{v:.3f}', ha='center', va='bottom', fontsize=9)

    # å­åœ– 2: ä¸¦è¡Œæ•ˆç‡ (è¶Šé«˜è¶Šå¥½)
    axes[1].bar(x_pos, parallel_values, color='gold', alpha=0.8)
    axes[1].set_ylabel('Efficiency (%)', fontsize=12)
    axes[1].set_title('Parallel Efficiency', fontsize=14, fontweight='bold')
    axes[1].set_xticks(x_pos)
    axes[1].set_xticklabels(names, rotation=45, ha='right')
    axes[1].grid(axis='y', alpha=0.3)
    axes[1].set_ylim(0, 100)

    for i, v in enumerate(parallel_values):
        axes[1].text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom', fontsize=9)

    # å­åœ– 3: è¨˜æ†¶é«”ä½¿ç”¨
    if any(memory_values):
        axes[2].bar(x_pos, memory_values, color='tomato', alpha=0.8)
        axes[2].set_ylabel('Memory (MB)', fontsize=12)
        axes[2].set_title('Memory Usage', fontsize=14, fontweight='bold')
        axes[2].set_xticks(x_pos)
        axes[2].set_xticklabels(names, rotation=45, ha='right')
        axes[2].grid(axis='y', alpha=0.3)

        for i, v in enumerate(memory_values):
            if v > 0:
                axes[2].text(i, v + max(memory_values)*0.02, f'{v:.1f}MB',
                            ha='center', va='bottom', fontsize=9)
    else:
        axes[2].text(0.5, 0.5, 'Memory data not available\n(psutil not installed)',
                    ha='center', va='center', fontsize=12, transform=axes[2].transAxes)
        axes[2].set_xticks([])
        axes[2].set_yticks([])

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ… åœ–è¡¨ 2 å·²ä¿å­˜: {output_path}")


def run_test_suite(suite_name: str, test_configs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    åŸ·è¡Œæ¸¬è©¦å¥—ä»¶
    """
    print(f"\n{'#'*80}")
    print(f"# æ¸¬è©¦å¥—ä»¶: {suite_name}")
    print(f"# æ¸¬è©¦æ•¸é‡: {len(test_configs)}")
    print(f"{'#'*80}\n")

    results = []

    for idx, config in enumerate(test_configs, 1):
        test_id = f"{suite_name}_{idx:02d}"
        result = run_single_test(config, test_id)
        results.append(result)

        # å»¶é²ä»¥é‡‹æ”¾è³‡æº
        if idx < len(test_configs):
            print("\nâ¸ï¸  ç­‰å¾… 5 ç§’å¾Œç¹¼çºŒä¸‹ä¸€å€‹æ¸¬è©¦...\n")
            time.sleep(5)

    return results


def generate_summary_report(all_results: Dict[str, List[Dict[str, Any]]], output_path: Path):
    """
    ç”Ÿæˆç¶œåˆæ‘˜è¦å ±å‘Š
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("IndexTTS Streaming Performance Test - Summary Report\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")

        for suite_name, results in all_results.items():
            f.write(f"\n{'='*80}\n")
            f.write(f"Test Suite: {suite_name}\n")
            f.write(f"{'='*80}\n\n")

            for result in results:
                f.write(f"Test: {result['name']}\n")
                f.write(f"Description: {result['description']}\n")

                if result.get("error"):
                    f.write(f"âŒ Status: FAILED\n")
                    f.write(f"   Error: {result['error']}\n")
                else:
                    f.write(f"âœ… Status: SUCCESS\n")
                    m = result.get("metrics", {})

                    if m.get("ttfb"):
                        f.write(f"   TTFB: {m['ttfb']:.2f}s\n")
                    if m.get("total_time"):
                        f.write(f"   Total Time: {m['total_time']:.2f}s\n")
                    if m.get("avg_gen_rate"):
                        f.write(f"   Avg Gen Rate: {m['avg_gen_rate']:.2f}x\n")
                    if m.get("overall_rtf"):
                        f.write(f"   Overall RTF: {m['overall_rtf']:.3f}\n")
                    if result.get("memory_usage_mb"):
                        f.write(f"   Memory Usage: {result['memory_usage_mb']:.1f} MB\n")

                f.write("\n")

        f.write("\n" + "=" * 80 + "\n")
        f.write("End of Report\n")
        f.write("=" * 80 + "\n")

    print(f"âœ… æ‘˜è¦å ±å‘Šå·²ä¿å­˜: {output_path}")


# ==================== ä¸»ç¨‹å¼ ====================

def main():
    print("\n" + "=" * 80)
    print("IndexTTS Streaming Performance - Comprehensive Test Suite")
    print("=" * 80 + "\n")

    # æª¢æŸ¥æ¸¬è©¦è…³æœ¬
    if not TEST_SCRIPT.exists():
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ¸¬è©¦è…³æœ¬ {TEST_SCRIPT}")
        sys.exit(1)

    # æª¢æŸ¥åƒè€ƒéŸ³æª”
    missing_audio = []
    for audio_file in ["voice_06.wav", "voice_07.wav"]:
        if not (REF_AUDIO_DIR / audio_file).exists():
            missing_audio.append(audio_file)

    if missing_audio:
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°åƒè€ƒéŸ³æª”: {', '.join(missing_audio)}")
        print(f"   é æœŸè·¯å¾‘: {REF_AUDIO_DIR}")
        sys.exit(1)

    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦å¥—ä»¶
    all_results = {}

    print("\nğŸ“‹ æ¸¬è©¦è¨ˆåŠƒ:")
    print("   1ï¸âƒ£  Voice Comparison (voice_06 vs voice_07)")
    print("   2ï¸âƒ£  Speed Strategy Comparison (Pre/Post/Hybrid)")
    print("   3ï¸âƒ£  Version & Mode Comparison (v1/v2, Streaming/Non-streaming)")
    print()

    # Test Suite 1
    print("\n" + "ğŸ”¹" * 40)
    suite_1_results = run_test_suite("Suite1_Voice_Comparison", TEST_SUITE_1)
    all_results["Suite1_Voice_Comparison"] = suite_1_results

    # Test Suite 2
    print("\n" + "ğŸ”¹" * 40)
    suite_2_results = run_test_suite("Suite2_Speed_Strategy", TEST_SUITE_2)
    all_results["Suite2_Speed_Strategy"] = suite_2_results

    # Test Suite 3
    print("\n" + "ğŸ”¹" * 40)
    suite_3_results = run_test_suite("Suite3_Version_Mode", TEST_SUITE_3)
    all_results["Suite3_Version_Mode"] = suite_3_results

    # ç”Ÿæˆå ±å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š ç”Ÿæˆå ±å‘Šä¸­...")
    print("=" * 80 + "\n")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆä½µæ‰€æœ‰çµæœ
    flat_results = []
    for results in all_results.values():
        flat_results.extend(results)

    # CSV
    csv_path = OUTPUT_DIR / f"benchmark_output_{timestamp}.csv"
    save_results_csv(flat_results, csv_path)

    # JSON
    json_path = OUTPUT_DIR / f"benchmark_output_{timestamp}.json"
    save_results_json(all_results, json_path)

    # è¦–è¦ºåŒ– 1 (æ‰€æœ‰æ¸¬è©¦)
    viz1_path = OUTPUT_DIR / f"performance_comparison_{timestamp}.png"
    generate_visualization_1(flat_results, viz1_path)

    # è¦–è¦ºåŒ– 2 (æ‰€æœ‰æ¸¬è©¦)
    viz2_path = OUTPUT_DIR / f"efficiency_analysis_{timestamp}.png"
    generate_visualization_2(flat_results, viz2_path)

    # æ‘˜è¦å ±å‘Š
    summary_path = OUTPUT_DIR / f"summary_report_{timestamp}.txt"
    generate_summary_report(all_results, summary_path)

    # æœ€çµ‚ç¸½çµ
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆ!")
    print("=" * 80)
    print(f"\nğŸ“‚ è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   â€¢ CSV æ•¸æ“šè¡¨æ ¼: {csv_path.name}")
    print(f"   â€¢ JSON è©³ç´°æ—¥èªŒ: {json_path.name}")
    print(f"   â€¢ æ€§èƒ½æ¯”è¼ƒåœ–è¡¨: {viz1_path.name}")
    print(f"   â€¢ æ•ˆç‡åˆ†æåœ–è¡¨: {viz2_path.name}")
    print(f"   â€¢ æ‘˜è¦å ±å‘Š: {summary_path.name}")
    print("\n" + "=" * 80 + "\n")


if __name__ == "__main__":
    main()
