#!/usr/bin/env python3
"""
IndexTTS Streaming Test with Audio Output Support
==========================================
åŸºæ–¼ test_streaming.pyï¼Œæ·»åŠ äº†éŸ³æª”è¼¸å‡ºåŠŸèƒ½

æ–°å¢åƒæ•¸:
  --output PATH    ä¿å­˜ç”Ÿæˆçš„éŸ³æª”åˆ°æŒ‡å®šè·¯å¾‘ (WAV æ ¼å¼)
"""

import sys
import os
import time
import argparse
import threading
import queue
import warnings
import tempfile
import numpy as np
import sounddevice as sd
import soundfile as sf

# å¼•å…¥å¤–éƒ¨ä¾è³´
try:
    import pyrubberband as pyrb
    from opencc import OpenCC
    import librosa
except ImportError as e:
    print(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦å¥—ä»¶ {e.name}ã€‚è«‹ç¢ºä¿å·²å®‰è£ pyrubberband, librosa å’Œ opencc-python-reimplemented")
    sys.exit(1)

# ==================== 1. ç’°å¢ƒåˆå§‹åŒ– ====================
import runtime_setup

env_paths = runtime_setup.initialize(__file__)
INDEX_TTS_DIR = env_paths["INDEX_TTS_DIR"]

sys.path.append(INDEX_TTS_DIR)
sys.path.append(os.path.join(INDEX_TTS_DIR, "indextts"))

import torch

from indextts.infer_v2 import IndexTTS2
from indextts.infer import IndexTTS
from indextts.infer_streaming_patch import add_streaming_to_indextts

# ==================== 2. å…¨åŸŸè¨­å®šèˆ‡å·¥å…·å‡½æ•¸ ====================

cc = OpenCC('t2s')

def convert_to_simplified(text):
    return cc.convert(text)

def get_timestamp(start_time):
    return time.time() - start_time

def check_cuda():
    print(f"\n{'='*20} ç¡¬é«”ç‹€æ…‹ {'='*20}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬   : {torch.version.cuda}")
        print(f"GPU å‹è™Ÿ    : {torch.cuda.get_device_name(0)}")
        print(f"é¡¯å­˜ (ç¸½é‡) : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        torch.cuda.empty_cache()
    else:
        print("âš ï¸ CUDA ä¸å¯ç”¨, å°‡ä½¿ç”¨ CPU (é€Ÿåº¦æœƒå—å½±éŸ¿)")

def split_text_smart(text, target_length=20, min_length=6):
    punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€,.'
    max_length = int(target_length * 1.5)
    segments = []
    current_segment = ""

    for char in text:
        current_segment += char
        if len(current_segment) >= min_length and char in punctuation:
            clean_seg = current_segment.strip()
            if clean_seg: segments.append(clean_seg)
            current_segment = ""
        elif len(current_segment) >= max_length:
            clean_seg = current_segment.strip()
            if clean_seg: segments.append(clean_seg)
            current_segment = ""

    if current_segment.strip():
        segments.append(current_segment.strip())

    return [seg.lstrip(punctuation).strip() for seg in segments if seg.lstrip(punctuation).strip()]

def time_stretch_robust(audio_data, sample_rate, speed_factor, quality='speech'):
    try:
        if quality == 'speech':
            try:
                return pyrb.time_stretch(
                    audio_data,
                    sample_rate,
                    speed_factor,
                    rbargs={'-c': 6}
                )
            except Exception:
                pass

        return pyrb.time_stretch(audio_data, sample_rate, speed_factor)

    except Exception:
        return librosa.effects.time_stretch(audio_data, rate=speed_factor)

# ==================== 3. æ’­æ”¾å™¨é‚è¼¯ ====================

class AudioPlayer(threading.Thread):
    def __init__(self, sample_rate, speed_factor=1.0, save_output=False):
        super().__init__(daemon=True)
        self.queue = queue.Queue()
        self.active = threading.Event()
        self.active.set()
        self.sample_rate = sample_rate
        self.speed_factor = speed_factor
        self.save_output = save_output
        self.events = []
        self.start_ref_time = 0
        self.collected_audio = []  # æ”¶é›†æ‰€æœ‰éŸ³è¨Šç‰‡æ®µç”¨æ–¼ä¿å­˜

    def set_start_time(self, start_time):
        self.start_ref_time = start_time

    def put_chunk(self, audio_data, duration, chunk_id):
        self.queue.put((audio_data, duration, chunk_id))

    def stop(self):
        self.queue.put(None)
        self.join()

    def get_full_audio(self):
        """è¿”å›å®Œæ•´çš„éŸ³è¨Šæ•¸æ“šï¼ˆå·²æ‡‰ç”¨è®Šé€Ÿï¼‰"""
        if not self.collected_audio:
            return None
        return np.concatenate(self.collected_audio)

    def run(self):
        chunk_idx = 0
        print(f"[Player] æ’­æ”¾åŸ·è¡Œç·’å•Ÿå‹• (æ¡æ¨£ç‡: {self.sample_rate}, æ’­æ”¾å€é€Ÿ: {self.speed_factor}x)")

        while self.active.is_set():
            try:
                item = self.queue.get(timeout=0.5)
                if item is None: break

                audio_normalized, original_duration, chunk_id = item
                chunk_idx += 1

                # è®Šé€Ÿè™•ç†
                if abs(self.speed_factor - 1.0) > 0.01:
                    try:
                        audio_play = time_stretch_robust(audio_normalized, self.sample_rate, self.speed_factor)
                    except Exception as e:
                        print(f"âš ï¸ æ’­æ”¾æ™‚è®Šé€Ÿå¤±æ•—: {e}")
                        audio_play = audio_normalized
                else:
                    audio_play = audio_normalized

                # æ”¶é›†éŸ³è¨Šï¼ˆå¦‚æœéœ€è¦ä¿å­˜ï¼‰
                if self.save_output:
                    self.collected_audio.append(audio_play)

                # è¨˜éŒ„äº‹ä»¶
                play_start = get_timestamp(self.start_ref_time)
                self.events.append({
                    'event': 'play_start',
                    'chunk': chunk_id,
                    'timestamp': play_start,
                    'duration': original_duration
                })

                # æ’­æ”¾
                sd.play(audio_play, samplerate=self.sample_rate)
                sd.wait()

                self.queue.task_done()

            except queue.Empty:
                continue

        print("[Player] æ’­æ”¾åŸ·è¡Œç·’çµæŸ")

# ==================== 4. ä¸»ç¨‹å¼é‚è¼¯ ====================

def main():
    warnings.filterwarnings("ignore", category=FutureWarning)
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

    ref_audio_dir = os.path.join(INDEX_TTS_DIR, "examples")
    ref_audio_dist = {
        "voice_03.wav":  os.path.join(ref_audio_dir, "voice_03.wav"),
        "voice_06.wav":  os.path.join(ref_audio_dir, "voice_06.wav"),
        "voice_07.wav":  os.path.join(ref_audio_dir, "voice_07.wav"),
        "voice_11.wav":  os.path.join(ref_audio_dir, "voice_11.wav"),
        "é˜¿ç’‹.wav":  os.path.join(ref_audio_dir, "é˜¿ç’‹.wav"),
        "GY.wav":  os.path.join(ref_audio_dir, "GY.wav"),
        "DIDI.wav":  os.path.join(ref_audio_dir, "DIDI.wav"),
        "JADE.wav":  os.path.join(ref_audio_dir, "JADE.wav"),
        "Joneshong.wav":  os.path.join(ref_audio_dir, "Joneshong.wav"),
        "Sean.wav":  os.path.join(ref_audio_dir, "Sean.wav"),
    }

    # --- åƒæ•¸è§£æ ---
    parser = argparse.ArgumentParser(description="Index-TTS Streaming Test")
    parser.add_argument("--version", type=str, default="v2", choices=["v1", "v2"], help="æ¨¡å‹ç‰ˆæœ¬")
    parser.add_argument("--method", type=str, default="token", choices=["token", "word"], help="åˆ‡åˆ†æ–¹æ³• (v2 Only)")
    parser.add_argument("--model_dir", type=str, default=None, help="æ¨¡å‹è³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--ref_audio", type=str, default=ref_audio_dist["Joneshong.wav"], help="åƒè€ƒéŸ³é »è·¯å¾‘")
    parser.add_argument("--text", type=str, default=None, help="æ¸¬è©¦æ–‡æœ¬")
    parser.add_argument("--steps", type=int, default=5, help="æ“´æ•£æ¨¡å‹æ­¥æ•¸ (åƒ…åƒè€ƒ)")
    parser.add_argument("--warmup", action="store_true", help="æ˜¯å¦åŸ·è¡Œæ¨¡å‹é ç†±")

    parser.add_argument("--speed", type=float, default=1.0,
                        help="[å¾Œè™•ç†] æ’­æ”¾åŠ é€Ÿå€ç‡ (ç”Ÿæˆå¾Œæ‰åŠ é€Ÿï¼Œé è¨­ 1.0)")

    parser.add_argument("--pre_speed_ref", type=float, default=1.0,
                        help="[é è™•ç†] åƒè€ƒéŸ³æª”åŠ é€Ÿå€ç‡ (TTSç”Ÿæˆå‰å…ˆåŠ é€Ÿåƒè€ƒéŸ³æª”ï¼Œé è¨­ 1.0)")

    # æ–°å¢: éŸ³æª”è¼¸å‡ºåƒæ•¸
    parser.add_argument("--output", type=str, default=None,
                        help="ä¿å­˜ç”Ÿæˆçš„éŸ³æª”åˆ°æŒ‡å®šè·¯å¾‘ (WAV æ ¼å¼)")

    args = parser.parse_args()

    # --- æ–‡æœ¬è™•ç† ---
    default_text = (
        "åŠ‰ä½©çœŸåˆ†æï¼Œè¡Œæ”¿é™¢ã€Œé–‹æ°´é¾é ­ã€ï¼Œ9æœˆåˆæ–°é’å®‰é¬†ç¶ï¼ŒåŠå»¶é•·å°å…ˆè²·å¾Œè³£æ›å±‹æ—å‡ºå”®èˆŠå±‹çš„æœŸé™ï¼Œè§€æœ›çš„å¸‚å ´æ°›åœç¨æ¸›ï¼Œæˆ¿å¸‚äº¤æ˜“é‡å‡ºç¾å°å¹…æˆé•·ï¼Œ"
        "äº‹å¯¦ä¸Šï¼Œä»Šå¹´æˆ¿å¸‚çš„äº¤æ˜“çµæ§‹å·²å¾å»å¹´çš„åƒ¹é‡é½Šæšï¼Œåˆ°ä»Šå¹´çš„é‡ç¸®ã€åƒ¹æ ¼ç·©è·Œã€‚"
        "ç›®å‰æˆ¿åƒ¹çš„è·Œå¹…æ–¹é¢ï¼Œç›¸è¼ƒæ–¼å»å¹´é‚„æœ‰éå¸¸ä½å€‹ä½æ•¸çš„ä¸‹æ»‘ï¼Œé¡¯ç¤ºæˆ¿å¸‚è³£æ–¹å¯¦éš›ä¸Šæ²’æœ‰å‡ºè„«çš„å£“åŠ›ã€‚"
    )
    target_text = args.text if args.text else default_text
    text_simplified = convert_to_simplified(target_text)

    # --- é¡¯ç¤ºé…ç½® ---
    print(f"\n{'='*20} æ¸¬è©¦é…ç½® {'='*20}")
    print(f"ç‰ˆæœ¬: {args.version}")
    print(f"æ–¹æ³•: {args.method}")
    print(f"åƒè€ƒéŸ³æª”: {os.path.basename(args.ref_audio)}")
    print(f"--------------------")
    print(f"1. åƒè€ƒéŸ³æª”åŠ é€Ÿ (TTSæ¨¡ä»¿): {args.pre_speed_ref}x")
    print(f"2. æ’­æ”¾å¾Œè£½åŠ é€Ÿ (DSPè™•ç†): {args.speed}x")
    if args.output:
        print(f"3. è¼¸å‡ºè·¯å¾‘: {args.output}")
    print(f"--------------------")

    check_cuda()

    # --- åƒè€ƒéŸ³æª”é è™•ç† ---
    temp_file_obj = None
    actual_ref_audio_path = args.ref_audio

    if abs(args.pre_speed_ref - 1.0) > 0.01:
        print(f"\nâš¡ æ­£åœ¨åŸ·è¡Œåƒè€ƒéŸ³æª”é åŠ é€Ÿ (å€ç‡: {args.pre_speed_ref}x)...")
        try:
            y, sr = sf.read(args.ref_audio)
            if len(y.shape) > 1: y = np.mean(y, axis=1)

            y_fast = time_stretch_robust(y, sr, args.pre_speed_ref, quality='speech')

            tf = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
            sf.write(tf.name, y_fast, sr)
            tf.close()

            temp_file_obj = tf
            actual_ref_audio_path = tf.name

            print(f"  âœ“ é åŠ é€Ÿå®Œæˆ")
            print(f"  âœ“ æš«å­˜åƒè€ƒéŸ³æª”è·¯å¾‘: {actual_ref_audio_path}")

        except Exception as e:
            print(f"âŒ é åŠ é€Ÿè™•ç†å¤±æ•—: {e}")
            actual_ref_audio_path = args.ref_audio
            if temp_file_obj and os.path.exists(temp_file_obj.name):
                os.remove(temp_file_obj.name)
            temp_file_obj = None

    # --- è¼‰å…¥æ¨¡å‹ ---
    print("\n=== è¼‰å…¥æ¨¡å‹ä¸­... ===")
    start_load = time.time()

    tts_model = None
    sampling_rate = 22050

    if args.version == "v2":
        model_dir = args.model_dir or os.path.join(INDEX_TTS_DIR, "checkpoints_v2")
        config_path = os.path.join(model_dir, "config.yaml")
        sampling_rate = 22050

        tts_model = IndexTTS2(
            cfg_path=config_path,
            model_dir=model_dir,
            use_fp16=True,
            use_cuda_kernel=False,
            use_deepspeed=False,
            use_accel=False,
            use_torch_compile=False
        )
    else:
        model_dir = args.model_dir or os.path.join(INDEX_TTS_DIR, "checkpoints_v1.5")
        config_path = os.path.join(model_dir, "config.yaml")
        sampling_rate = 24000

        tts_model = IndexTTS(
            model_dir=model_dir,
            cfg_path=config_path,
            use_fp16=True,
            use_cuda_kernel=False
        )
        tts_model = add_streaming_to_indextts(tts_model)

    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ (è€—æ™‚: {time.time() - start_load:.2f}s)")

    # --- æ¨¡å‹é ç†± ---
    if args.warmup:
        print(f"\n{'='*20} ğŸ”¥ æ¨¡å‹é ç†± {'='*20}")
        print("æ­£åœ¨åŸ·è¡Œé ç†±...")
        warmup_start = time.time()
        warmup_text = "æ¸¬è©¦é ç†±ã€‚"
        try:
            if args.version == "v2":
                dummy_kwargs = {
                    "spk_audio_prompt": actual_ref_audio_path,
                    "text": convert_to_simplified(warmup_text),
                    "output_path": None,
                    "stream_return": True,
                    "interval_silence": 150,
                    "verbose": False,
                    "use_emo_text": False,
                    "emo_vector": None
                }
                if args.method == "token":
                    dummy_kwargs["max_text_tokens_per_segment"] = 68
                for _ in tts_model.infer(**dummy_kwargs): pass
            else:
                for _ in tts_model.infer_stream(actual_ref_audio_path, convert_to_simplified(warmup_text), verbose=False): pass

            if torch.cuda.is_available(): torch.cuda.synchronize()
            print(f"âœ… é ç†±å®Œæˆ (è€—æ™‚: {time.time() - warmup_start:.2f}s)")
        except Exception as e:
            print(f"âš ï¸ é ç†±éŒ¯èª¤: {e}")

    # --- æº–å‚™æ’­æ”¾å™¨ (å•Ÿç”¨éŸ³æª”æ”¶é›†) ---
    player = AudioPlayer(
        sample_rate=sampling_rate,
        speed_factor=args.speed,
        save_output=bool(args.output)  # å¦‚æœæœ‰è¼¸å‡ºè·¯å¾‘ï¼Œå•Ÿç”¨æ”¶é›†
    )
    player.start()

    # --- æº–å‚™ç”Ÿæˆ ---
    processing_queue = []
    if args.version == "v2" and args.method == "token":
        processing_queue.append((text_simplified, "full_text"))
    else:
        segments = split_text_smart(text_simplified)
        print(f"ğŸ“ æ‰‹å‹•åˆ‡åˆ†: å…± {len(segments)} æ®µ")
        for i, seg in enumerate(segments):
            processing_queue.append((seg, f"segment_{i+1}"))

    # --- é–‹å§‹ç”Ÿæˆ ---
    global_start_time = time.time()
    player.set_start_time(global_start_time)

    chunk_count = 0
    first_chunk_time = None
    generation_events = []
    speed_stats = []

    print(f"\n[ğŸš€ Start] é–‹å§‹ä¸²æµç”Ÿæˆ...")

    try:
        try:
            for text_input, label in processing_queue:
                print(f"[ğŸ¬ Gen] æ­£åœ¨è™•ç†: {label} ({len(text_input)}å­—)")

                audio_generator = None

                if args.version == "v2":
                    kwargs = {
                        "spk_audio_prompt": actual_ref_audio_path,
                        "text": text_input,
                        "output_path": None,
                        "stream_return": True,
                        "interval_silence": 150,
                        "verbose": False,
                        "use_emo_text": False,
                        "emo_vector": None
                    }
                    if args.method == "token":
                        kwargs["max_text_tokens_per_segment"] = 68
                    audio_generator = tts_model.infer(**kwargs)
                else:
                    audio_generator = tts_model.infer_stream(actual_ref_audio_path, text_input, verbose=False)

                t_last_chunk_finish = time.time()

                for audio_chunk in audio_generator:
                    t_now_abs = time.time()
                    t_now_rel = get_timestamp(global_start_time)
                    chunk_latency = t_now_abs - t_last_chunk_finish
                    t_last_chunk_finish = t_now_abs
                    chunk_count += 1

                    if isinstance(audio_chunk, list):
                        audio_chunk = torch.cat(audio_chunk, dim=-1) if len(audio_chunk) > 0 else torch.zeros(1)
                    audio_np = audio_chunk.cpu().numpy().squeeze()
                    audio_normalized = audio_np.astype(np.float32) / 32767.0
                    duration = audio_np.shape[-1] / sampling_rate

                    if chunk_latency > 0.01:
                        gen_rate = duration / chunk_latency
                        speed_stats.append(gen_rate)

                    if duration < 0.05: continue

                    if first_chunk_time is None:
                        first_chunk_time = t_now_rel
                        print(f"[âš¡ First Token] é¦–å€‹éŸ³è¨Šå·²ç”Ÿæˆ: {first_chunk_time:.2f}s")

                    generation_events.append({
                        'event': 'generate',
                        'chunk': chunk_count,
                        'timestamp': t_now_rel,
                        'duration': duration
                    })

                    player.put_chunk(audio_normalized, duration, chunk_count)
                    if duration > 0.1:
                        print(f"  -> [Queue] ç‰‡æ®µ {chunk_count} (éŸ³é•· {duration:.2f}s, è€—æ™‚ {chunk_latency:.2f}s, å€ç‡ {gen_rate:.2f}x)")
        except KeyboardInterrupt:
            print("\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·")
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

    finally:
        if temp_file_obj:
            try:
                if not temp_file_obj.closed:
                    temp_file_obj.close()

                if os.path.exists(temp_file_obj.name):
                    os.remove(temp_file_obj.name)
                    print(f"\nğŸ—‘ï¸ å·²æ¸…ç†æš«å­˜åƒè€ƒéŸ³æª”: {temp_file_obj.name}")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†æš«å­˜æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    total_gen_time = get_timestamp(global_start_time)
    print(f"\n[ğŸ Finish] æ‰€æœ‰ç”Ÿæˆä»»å‹™å®Œæˆ (ç¸½è€—æ™‚: {total_gen_time:.2f}s)")

    player.stop()

    # --- ä¿å­˜éŸ³æª” (æ–°å¢åŠŸèƒ½) ---
    if args.output:
        print(f"\n{'='*20} ğŸ’¾ ä¿å­˜éŸ³æª” {'='*20}")
        try:
            full_audio = player.get_full_audio()
            if full_audio is not None and len(full_audio) > 0:
                # ç¢ºä¿ç›®éŒ„å­˜åœ¨
                output_dir = os.path.dirname(args.output)
                if output_dir and not os.path.exists(output_dir):
                    os.makedirs(output_dir, exist_ok=True)

                # ä¿å­˜ WAV
                sf.write(args.output, full_audio, sampling_rate)
                file_size = os.path.getsize(args.output) / 1024 / 1024  # MB
                duration = len(full_audio) / sampling_rate

                print(f"âœ… éŸ³æª”å·²ä¿å­˜:")
                print(f"   è·¯å¾‘: {args.output}")
                print(f"   å¤§å°: {file_size:.2f} MB")
                print(f"   æ™‚é•·: {duration:.2f} s")
                print(f"   æ¡æ¨£ç‡: {sampling_rate} Hz")
            else:
                print(f"âš ï¸ ç„¡éŸ³è¨Šæ•¸æ“šå¯ä¿å­˜")
        except Exception as e:
            print(f"âŒ ä¿å­˜éŸ³æª”å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()

    # --- çµ±è¨ˆå ±å‘Š ---
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç¶œåˆçµ±è¨ˆå ±å‘Š")
    print(f"{'='*80}")

    print(f"\nğŸ”§ åŸ·è¡Œåƒæ•¸ (Arguments):")
    for k, v in vars(args).items():
        print(f"  â€¢ {k:<12} : {v}")

    print(f"\nğŸµ åƒè€ƒéŸ³è¨Šè³‡è¨Š (Ref Audio):")
    if os.path.exists(args.ref_audio):
        try:
            file_size_bytes = os.path.getsize(args.ref_audio)
            file_size_mb = file_size_bytes / (1024 * 1024)
            sf_info = sf.info(args.ref_audio)
            duration = sf_info.duration
            samplerate = sf_info.samplerate
            subtype = sf_info.subtype
            bitrate_kbps = (file_size_bytes * 8) / duration / 1000 if duration > 0 else 0

            print(f"  â€¢ æª”å (File)    : {os.path.basename(args.ref_audio)}")
            print(f"  â€¢ å¤§å° (Size)    : {file_size_mb:.2f} MB")
            print(f"  â€¢ ç§’æ•¸ (Length)  : {duration:.2f} s")
            print(f"  â€¢ æ ¼å¼ (Format)  : {sf_info.format} ({subtype})")
            print(f"  â€¢ æ¡æ¨£ (Rate)    : {samplerate} Hz")
            print(f"  â€¢ ç¢¼ç‡ (Bitrate) : {bitrate_kbps:.0f} kbps")
        except Exception as e:
            print(f"  âš ï¸ ç„¡æ³•è®€å–éŸ³è¨Šè³‡è¨Š: {e}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {args.ref_audio}")

    print(f"\nğŸš€ æ•ˆèƒ½æŒ‡æ¨™:")
    print(f"  â€¢ é¦–æ¬¡éŸ¿æ‡‰   : {first_chunk_time if first_chunk_time else 'N/A'}")
    print(f"  â€¢ ç¸½è€—æ™‚     : {total_gen_time:.2f} s")

    if speed_stats:
        avg_rate = np.mean(speed_stats)
        max_rate = np.max(speed_stats)
        min_rate = np.min(speed_stats)
        print(f"  â€¢ ç”Ÿæˆå€ç‡ (Audio/Process Speed):")
        print(f"    (æ•¸å€¼ > 1.0 ä»£è¡¨ç”Ÿæˆé€Ÿåº¦æ¯”æ’­æ”¾é€Ÿåº¦å¿«)")
        print(f"      Avg : {avg_rate:.2f} x")
        print(f"      Max : {max_rate:.2f} x")
        print(f"      Min : {min_rate:.2f} x")

        total_audio_len = sum(e['duration'] for e in generation_events)
        overall_rtf = total_gen_time / total_audio_len if total_audio_len > 0 else 0
        print(f"  â€¢ æ•´é«”å¯¦æ™‚ç‡ (RTF): {overall_rtf:.3f} (è¶Šä½è¶Šå¥½)")
    else:
        print(f"  â€¢ ç”Ÿæˆå€ç‡: N/A")

    # ä¸¦è¡Œæ•ˆç‡
    overlap_count = 0
    playback_events = player.events
    if len(generation_events) > 1 and len(playback_events) > 0:
        for gen_event in generation_events[1:]:
            g_time = gen_event['timestamp']
            for play_event in playback_events:
                p_start = play_event['timestamp']
                p_end = p_start + (play_event['duration'] / args.speed)
                if p_start <= g_time <= p_end:
                    overlap_count += 1
                    break
        efficiency = (overlap_count / max(chunk_count - 1, 1)) * 100
        print(f"  â€¢ ä¸¦è¡Œæ•ˆç‡ (Parallel): {efficiency:.1f}% ({overlap_count}/{chunk_count-1} chunks overlapped)")

    print(f"{'='*80}\n")
    print("Done.")

if __name__ == "__main__":
    main()
