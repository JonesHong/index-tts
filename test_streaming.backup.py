import sys
import os
import time
import argparse
import threading
import queue
import gc
import warnings
import numpy as np
import sounddevice as sd
import soundfile as sf

# å¼•å…¥å¤–éƒ¨ä¾è³´
try:
    import pyrubberband as pyrb
    from opencc import OpenCC
except ImportError as e:
    print(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦å¥—ä»¶ {e.name}ã€‚è«‹ç¢ºä¿å·²å®‰è£ pyrubberband å’Œ opencc-python-reimplemented")
    sys.exit(1)

# ==================== 1. ç’°å¢ƒåˆå§‹åŒ– (ä½¿ç”¨ runtime_setup) ====================
import runtime_setup

# åˆå§‹åŒ–ä¸¦å–å¾—è·¯å¾‘
env_paths = runtime_setup.initialize(__file__)
INDEX_TTS_DIR = env_paths["INDEX_TTS_DIR"]

# è¨­å®š Python Path
sys.path.append(INDEX_TTS_DIR)
sys.path.append(os.path.join(INDEX_TTS_DIR, "indextts"))

# å¿…é ˆåœ¨å°å…¥ torch ä¹‹å‰è¨­å®š,é¿å… DeepSpeed ç·¨è­¯æª¢æŸ¥
import torch

# IndexTTS æ¨¡çµ„å°å…¥ (å¿…é ˆåœ¨ sys.path è¨­å®šå¾Œ)
from indextts.infer_v2 import IndexTTS2
from indextts.infer import IndexTTS
from indextts.infer_streaming_patch import add_streaming_to_indextts

# ==================== 2. å…¨åŸŸè¨­å®šèˆ‡å·¥å…·å‡½æ•¸ ====================

# ç¹ç°¡è½‰æ›
cc = OpenCC('t2s')

def convert_to_simplified(text):
    return cc.convert(text)

def get_timestamp(start_time):
    """ç²å–ç›¸å°æ™‚é–“æˆ³è¨˜ï¼ˆç§’ï¼‰"""
    return time.time() - start_time

def check_cuda():
    """æª¢æŸ¥ä¸¦æ‰“å° CUDA ç‹€æ…‹"""
    print(f"\n{'='*20} ç¡¬é«”ç‹€æ…‹ {'='*20}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬   : {torch.version.cuda}")
        print(f"GPU å‹è™Ÿ    : {torch.cuda.get_device_name(0)}")
        print(f"é¡¯å­˜ (ç¸½é‡) : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        torch.cuda.empty_cache()
    else:
        print("âš ï¸ CUDA ä¸å¯ç”¨, å°‡ä½¿ç”¨ CPU (é€Ÿåº¦æœƒå—å½±éŸ¿)")

# æ–‡å­—åˆ‡åˆ†é‚è¼¯
def split_text_smart(text, target_length=20, min_length=6):
    """å„ªåŒ–ç‰ˆåˆ‡åˆ†ï¼šå„ªå…ˆåœ¨æ¨™é»åˆ‡åˆ†"""
    punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€,.'
    max_length = int(target_length * 1.5)
    segments = []
    current_segment = ""
    
    for char in text:
        current_segment += char
        if len(current_segment) >= min_length and char in punctuation:
            clean_seg = current_segment.strip()
            if clean_seg: segments.append(clean_seg)
            current_segment = ""
        elif len(current_segment) >= max_length:
            clean_seg = current_segment.strip()
            if clean_seg: segments.append(clean_seg)
            current_segment = ""
            
    if current_segment.strip():
        segments.append(current_segment.strip())
        
    # äºŒæ¬¡æ¸…ç†
    return [seg.lstrip(punctuation).strip() for seg in segments if seg.lstrip(punctuation).strip()]

# ==================== 3. æ’­æ”¾å™¨é‚è¼¯ (ç¨ç«‹åŸ·è¡Œç·’) ====================

class AudioPlayer(threading.Thread):
    def __init__(self, sample_rate, speed_factor=1.0):
        super().__init__(daemon=True)
        self.queue = queue.Queue()
        self.active = threading.Event()
        self.active.set()
        self.sample_rate = sample_rate
        self.speed_factor = speed_factor
        self.events = [] # è¨˜éŒ„æ’­æ”¾äº‹ä»¶
        self.start_ref_time = 0

    def set_start_time(self, start_time):
        self.start_ref_time = start_time

    def put_chunk(self, audio_data, duration, chunk_id):
        self.queue.put((audio_data, duration, chunk_id))

    def stop(self):
        self.queue.put(None) # çµæŸä¿¡è™Ÿ
        self.join()

    def run(self):
        chunk_idx = 0
        print(f"[Player] æ’­æ”¾åŸ·è¡Œç·’å•Ÿå‹• (æ¡æ¨£ç‡: {self.sample_rate}, å€é€Ÿ: {self.speed_factor})")
        
        while self.active.is_set():
            try:
                item = self.queue.get(timeout=0.5)
                if item is None: break # æ”¶åˆ°çµæŸä¿¡è™Ÿ

                audio_normalized, original_duration, chunk_id = item
                chunk_idx += 1
                
                # è®Šé€Ÿè™•ç† (pyrubberband) - æ”¾åœ¨é€™è£¡åšæ˜¯ç‚ºäº†ä¸é˜»å¡ç”ŸæˆåŸ·è¡Œç·’
                if abs(self.speed_factor - 1.0) > 0.01:
                    # ä½¿ç”¨å­—å…¸åƒæ•¸å„ªåŒ–éŸ³è³ª (é˜²æ­¢æ··éŸ¿)
                    try:
                        audio_play = pyrb.time_stretch(
                            audio_normalized, 
                            self.sample_rate, 
                            self.speed_factor,
                            rbargs={'-c': 6} # Crispness 6 (High)
                        )
                    except:
                        # Fallback
                        audio_play = pyrb.time_stretch(audio_normalized, self.sample_rate, self.speed_factor)
                else:
                    audio_play = audio_normalized

                # è¨˜éŒ„é–‹å§‹
                play_start = get_timestamp(self.start_ref_time)
                self.events.append({'event': 'play_start', 'chunk': chunk_id, 'timestamp': play_start, 'duration': original_duration})
                
                print(f"[ğŸ”Š Play] ç‰‡æ®µ {chunk_id} é–‹å§‹æ’­æ”¾ (åŸå§‹æ™‚é•· {original_duration:.2f}s)")
                
                # æ’­æ”¾ (é˜»å¡ç›´åˆ°æ’­å®Œ)
                sd.play(audio_play, samplerate=self.sample_rate)
                sd.wait()

                # è¨˜éŒ„çµæŸ
                self.queue.task_done()
                
            except queue.Empty:
                continue
        print("[Player] æ’­æ”¾åŸ·è¡Œç·’çµæŸ")

# ==================== 4. ä¸»ç¨‹å¼é‚è¼¯ ====================

def main():
    # --- 0. å±è”½å¹²æ“¾è¨Šæ¯ ---
    warnings.filterwarnings("ignore", category=FutureWarning) 
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
    ref_audio_dir = os.path.join(INDEX_TTS_DIR, "examples")
    ref_audio_dist = {
        "voice_03.wav":  os.path.join(ref_audio_dir, "voice_03.wav"),
        "voice_06.wav":  os.path.join(ref_audio_dir, "voice_06.wav"),
        "voice_07.wav":  os.path.join(ref_audio_dir, "voice_07.wav"),
        "voice_11.wav":  os.path.join(ref_audio_dir, "voice_11.wav"),
        "é˜¿ç’‹.wav":  os.path.join(ref_audio_dir, "é˜¿ç’‹.wav"),
        "GY.wav":  os.path.join(ref_audio_dir, "GY.wav"),
        "DIDI.wav":  os.path.join(ref_audio_dir, "DIDI.wav"),
        "JADE.wav":  os.path.join(ref_audio_dir, "JADE.wav"),
        "Joneshong.wav":  os.path.join(ref_audio_dir, "Joneshong.wav"),
        "Sean.wav":  os.path.join(ref_audio_dir, "Sean.wav"),
    }
    # --- åƒæ•¸è§£æ ---
    parser = argparse.ArgumentParser(description="Index-TTS Streaming Test")
    parser.add_argument("--version", type=str, default="v2", choices=["v1", "v2"], help="æ¨¡å‹ç‰ˆæœ¬")
    parser.add_argument("--method", type=str, default="token", choices=["token", "word"], help="åˆ‡åˆ†æ–¹æ³• (v2 Only)")
    parser.add_argument("--model_dir", type=str, default=None, help="æ¨¡å‹è³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--ref_audio", type=str, default=ref_audio_dist["voice_06.wav"], help="åƒè€ƒéŸ³é »è·¯å¾‘")
    parser.add_argument("--speed", type=float, default=1.3, help="æ’­æ”¾èªé€Ÿ")
    parser.add_argument("--text", type=str, default=None, help="æ¸¬è©¦æ–‡æœ¬")
    parser.add_argument("--steps", type=int, default=5, help="æ“´æ•£æ¨¡å‹æ­¥æ•¸ (åƒ…åƒè€ƒ)")
    parser.add_argument("--warmup", action="store_true", help="æ˜¯å¦åŸ·è¡Œæ¨¡å‹é ç†±") # <--- æ–°å¢åƒæ•¸
    
    args = parser.parse_args()

    # --- æ–‡æœ¬è™•ç† ---
    default_text = (
        # "ä¸€åè‡ªç¨±å°å¤§å¤§æ°£ç³»å­¸ç”Ÿçš„ç¶²å‹åœ¨è‡‰æ›¸ã€Œé»‘ç‰¹å¸å¤§ã€ç«‹ä¸‹è±ªè¨€ï¼Œé æ¸¬å°åŒ—å¸‚åä¸€æœˆåäºŒæ—¥è‡³åå››æ—¥è‡³å°‘æœƒå› é³³å‡°é¢±é¢¨æ”¾å‡å…©å¤©ï¼Œ"
        # "ä¸¦ä»¥ä¸‰ç™¾ä»½é›æ’çå¥¶ä½œç‚ºå¤±æº–æ™‚çš„ç¥­å“ã€‚çµæœé€™ä½å­¸ç”Ÿè¢«ç¾å¯¦ç‹ ç‹ æ‰“è‡‰ï¼Œæœ€çµ‚ä»–ä¹Ÿå¯¦ç¾æ‰¿è«¾ï¼Œ"
        # "å®£å¸ƒåä¸€æœˆåå…­æ—¥ä¸­åˆå°±åœ¨å°å¤§æ ¡åœ’å…§ç™¼é€é›æ’ã€çå¥¶ï¼Œè®“ç¶²å‹æ¶ç•™è¨€å¡ä½ã€‚"
        "åŠ‰ä½©çœŸåˆ†æï¼Œè¡Œæ”¿é™¢ã€Œé–‹æ°´é¾é ­ã€ï¼Œ9æœˆåˆæ–°é’å®‰é¬†ç¶ï¼ŒåŠå»¶é•·å°å…ˆè²·å¾Œè³£æ›å±‹æ—å‡ºå”®èˆŠå±‹çš„æœŸé™ï¼Œè§€æœ›çš„å¸‚å ´æ°›åœç¨æ¸›ï¼Œæˆ¿å¸‚äº¤æ˜“é‡å‡ºç¾å°å¹…æˆé•·ï¼Œ"
        "äº‹å¯¦ä¸Šï¼Œä»Šå¹´æˆ¿å¸‚çš„äº¤æ˜“çµæ§‹å·²å¾å»å¹´çš„åƒ¹é‡é½Šæšï¼Œåˆ°ä»Šå¹´çš„é‡ç¸®ã€åƒ¹æ ¼ç·©è·Œã€‚"
        "ç›®å‰æˆ¿åƒ¹çš„è·Œå¹…æ–¹é¢ï¼Œç›¸è¼ƒæ–¼å»å¹´é‚„æœ‰éå¸¸ä½å€‹ä½æ•¸çš„ä¸‹æ»‘ï¼Œé¡¯ç¤ºæˆ¿å¸‚è³£æ–¹å¯¦éš›ä¸Šæ²’æœ‰å‡ºè„«çš„å£“åŠ›ã€‚"
    )
    target_text = args.text if args.text else default_text
    text_simplified = convert_to_simplified(target_text)

    # --- é¡¯ç¤ºé…ç½® ---
    print(f"\n{'='*20} æ¸¬è©¦é…ç½® {'='*20}")
    print(f"ç‰ˆæœ¬: {args.version}")
    print(f"æ–¹æ³•: {args.method}")
    print(f"èªé€Ÿ: {args.speed}")
    print(f"é ç†±: {'é–‹å•Ÿ' if args.warmup else 'é—œé–‰'}")
    print(f"åŸæ–‡: {target_text[:30]}...")
    
    check_cuda()

    # --- è¼‰å…¥æ¨¡å‹ ---
    print("\n=== è¼‰å…¥æ¨¡å‹ä¸­... ===")
    start_load = time.time()
    
    tts_model = None
    sampling_rate = 22050 

    if args.version == "v2":
        model_dir = args.model_dir or os.path.join(INDEX_TTS_DIR, "checkpoints_v2")
        config_path = os.path.join(model_dir, "config.yaml")
        sampling_rate = 22050
        
        tts_model = IndexTTS2(
            cfg_path=config_path,
            model_dir=model_dir,
            use_fp16=True,
            use_cuda_kernel=False,
            use_deepspeed=False,
            use_accel=False,
            use_torch_compile=False
        )
    else: # v1
        model_dir = args.model_dir or os.path.join(INDEX_TTS_DIR, "checkpoints_v1.5")
        config_path = os.path.join(model_dir, "config.yaml")
        sampling_rate = 24000
        
        tts_model = IndexTTS(
            model_dir=model_dir,
            cfg_path=config_path,
            use_fp16=True,
            use_cuda_kernel=False
        )
        tts_model = add_streaming_to_indextts(tts_model)

    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ (è€—æ™‚: {time.time() - start_load:.2f}s)")

    # ==================== æ–°å¢ï¼šæ¨¡å‹é ç†± ====================
    if args.warmup:
        print(f"\n{'='*20} ğŸ”¥ æ¨¡å‹é ç†± {'='*20}")
        print("æ­£åœ¨åŸ·è¡Œé ç†± (Run dry-run)...")
        warmup_start = time.time()
        warmup_text = "ä½ å¥½ï¼Œé€™æ˜¯ä¸€æ®µç”¨ä¾†é ç†±æ¨¡å‹çš„æ¸¬è©¦æ–‡æœ¬ã€‚"
        
        # ç°¡å–®è·‘ä¸€æ¬¡ç”Ÿæˆï¼Œä¸æ”¾å…¥æ’­æ”¾éšŠåˆ—
        try:
            # æº–å‚™åƒæ•¸
            if args.version == "v2":
                dummy_kwargs = {
                    "spk_audio_prompt": args.ref_audio,
                    "text": convert_to_simplified(warmup_text),
                    "output_path": None,
                    "stream_return": True,
                    "interval_silence": 150,
                    "verbose": False,
                    "use_emo_text": False,
                    "emo_vector": None
                }
                if args.method == "token":
                    dummy_kwargs["max_text_tokens_per_segment"] = 68
                
                # æ¶ˆè€—ç”Ÿæˆå™¨ (è¨ˆç®—ä½†ä¸ä½¿ç”¨)
                for _ in tts_model.infer(**dummy_kwargs):
                    pass
            else:
                # v1
                for _ in tts_model.infer_stream(args.ref_audio, convert_to_simplified(warmup_text), verbose=False):
                    pass
            
            # å¼·åˆ¶åŒæ­¥ CUDA ç¢ºä¿é ç†±çœŸçš„åšå®Œäº†
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                
            print(f"âœ… é ç†±å®Œæˆ (è€—æ™‚: {time.time() - warmup_start:.2f}s)")
        except Exception as e:
            print(f"âš ï¸ é ç†±éç¨‹ç™¼ç”ŸéŒ¯èª¤ (å·²ç•¥é): {e}")
    # =======================================================

    # --- æº–å‚™æ’­æ”¾å™¨ ---
    player = AudioPlayer(sample_rate=sampling_rate, speed_factor=args.speed)
    player.start()

    # --- æº–å‚™ç”Ÿæˆ ---
    processing_queue = [] 
    if args.version == "v2" and args.method == "token":
        processing_queue.append((text_simplified, "full_text"))
    else:
        segments = split_text_smart(text_simplified)
        print(f"ğŸ“ æ‰‹å‹•åˆ‡åˆ†: å…± {len(segments)} æ®µ")
        for i, seg in enumerate(segments):
            processing_queue.append((seg, f"segment_{i+1}"))

    # --- é–‹å§‹ç”Ÿæˆè¿´åœˆ ---
    # æ³¨æ„ï¼šé€™è£¡æ‰é–‹å§‹æ­£å¼è¨ˆæ™‚
    global_start_time = time.time()
    player.set_start_time(global_start_time)
    
    chunk_count = 0
    first_chunk_time = None
    generation_events = []
    
    # [çµ±è¨ˆä¿®æ­£] æ”¹ç”¨ Audio/Sec (ç”Ÿæˆå€ç‡)
    speed_stats = [] 

    print(f"\n[ğŸš€ Start] é–‹å§‹ä¸²æµç”Ÿæˆ (è¨ˆæ™‚é–‹å§‹)...")

    for text_input, label in processing_queue:
        print(f"[ğŸ¬ Gen] æ­£åœ¨è™•ç†: {label} ({len(text_input)}å­—)")

        audio_generator = None
        
        if args.version == "v2":
            kwargs = {
                "spk_audio_prompt": args.ref_audio,
                "text": text_input,
                "output_path": None,
                "stream_return": True,
                "interval_silence": 150,
                "verbose": False,
                "use_emo_text": False,
                "emo_vector": None
            }
            if args.method == "token":
                kwargs["max_text_tokens_per_segment"] = 68
            audio_generator = tts_model.infer(**kwargs)
        else:
            audio_generator = tts_model.infer_stream(args.ref_audio, text_input, verbose=False)

        # è¨ˆæ™‚åˆå§‹åŒ–
        t_last_chunk_finish = time.time()

        for audio_chunk in audio_generator:
            # å–å¾—ç•¶å‰æ™‚é–“
            t_now_abs = time.time()
            t_now_rel = get_timestamp(global_start_time)
            
            # è¨ˆç®—æœ¬æ¬¡ç”Ÿæˆè€—æ™‚ (Latency)
            chunk_latency = t_now_abs - t_last_chunk_finish
            t_last_chunk_finish = t_now_abs 
            
            chunk_count += 1

            # è™•ç†éŸ³è¨Š
            if isinstance(audio_chunk, list):
                audio_chunk = torch.cat(audio_chunk, dim=-1) if len(audio_chunk) > 0 else torch.zeros(1)
            audio_np = audio_chunk.cpu().numpy().squeeze()
            audio_normalized = audio_np.astype(np.float32) / 32767.0
            duration = audio_np.shape[-1] / sampling_rate
            
            # è¨ˆç®—ç”Ÿæˆå€ç‡ (Audio Sec / Process Sec)
            # æ•¸å€¼ > 1 ä»£è¡¨ç”Ÿæˆæ¯”æ’­æ”¾å¿« (ç†æƒ³æƒ…æ³)
            if chunk_latency > 0.01:
                gen_rate = duration / chunk_latency
                speed_stats.append(gen_rate)
            else:
                gen_rate = 0
            
            if duration < 0.05: continue 

            if first_chunk_time is None:
                first_chunk_time = t_now_rel
                print(f"[âš¡ First Token] é¦–å€‹éŸ³è¨Šå·²ç”Ÿæˆ: {first_chunk_time:.2f}s")

            generation_events.append({
                'event': 'generate',
                'chunk': chunk_count,
                'timestamp': t_now_rel,
                'duration': duration
            })

            # é€™è£¡é¡¯ç¤º Gen Rate (å€ç‡)
            if duration > 0.1:
                print(f"  -> [Queue] ç‰‡æ®µ {chunk_count} (éŸ³é•· {duration:.2f}s, è€—æ™‚ {chunk_latency:.2f}s, å€ç‡ {gen_rate:.2f}x)")
                player.put_chunk(audio_normalized, duration, chunk_count)
            else:
                print(f"  -> [Mute ] éœéŸ³ç‰‡æ®µ {chunk_count} (éŸ³é•· {duration:.2f}s)")
                player.put_chunk(audio_normalized, duration, chunk_count)

    total_gen_time = get_timestamp(global_start_time)
    print(f"\n[ğŸ Finish] æ‰€æœ‰ç”Ÿæˆä»»å‹™å®Œæˆ (ç¸½è€—æ™‚: {total_gen_time:.2f}s)")
    
    player.stop()

    # ==================== 5. ç¶œåˆçµ±è¨ˆå ±å‘Š ====================
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç¶œåˆçµ±è¨ˆå ±å‘Š")
    print(f"{'='*80}")

    # --- A. åƒæ•¸é…ç½® ---
    print(f"ğŸ”§ åŸ·è¡Œåƒæ•¸ (Arguments):")
    for k, v in vars(args).items():
        print(f"  â€¢ {k:<12} : {v}")
    
    # --- B. åƒè€ƒéŸ³è¨Šåˆ†æ ---
    print(f"\nğŸµ åƒè€ƒéŸ³è¨Šè³‡è¨Š (Ref Audio):")
    if os.path.exists(args.ref_audio):
        try:
            file_size_bytes = os.path.getsize(args.ref_audio)
            file_size_mb = file_size_bytes / (1024 * 1024)
            sf_info = sf.info(args.ref_audio)
            duration = sf_info.duration
            samplerate = sf_info.samplerate
            subtype = sf_info.subtype
            bitrate_kbps = (file_size_bytes * 8) / duration / 1000 if duration > 0 else 0
            
            print(f"  â€¢ æª”å (File)    : {os.path.basename(args.ref_audio)}")
            print(f"  â€¢ å¤§å° (Size)    : {file_size_mb:.2f} MB")
            print(f"  â€¢ ç§’æ•¸ (Length)  : {duration:.2f} s")
            print(f"  â€¢ æ ¼å¼ (Format)  : {sf_info.format} ({subtype})")
            print(f"  â€¢ æ¡æ¨£ (Rate)    : {samplerate} Hz")
            print(f"  â€¢ ç¢¼ç‡ (Bitrate) : {bitrate_kbps:.0f} kbps")
        except Exception as e:
            print(f"  âš ï¸ ç„¡æ³•è®€å–éŸ³è¨Šè³‡è¨Š: {e}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {args.ref_audio}")

    # --- C. æ•ˆèƒ½çµ±è¨ˆ ---
    print(f"{'-'*40}")
    print(f"ğŸš€ æ•ˆèƒ½æŒ‡æ¨™ (Performance):")
    print(f"  â€¢ é¦–æ¬¡éŸ¿æ‡‰ (TTFT): {first_chunk_time if first_chunk_time else 'N/A'}")
    print(f"  â€¢ ç¸½è€—æ™‚   (Total): {total_gen_time:.2f} s")
    print(f"  â€¢ ç¸½ç‰‡æ®µæ•¸ (Chunks): {chunk_count}")

    # [ä¿®æ­£] é¡¯ç¤ºç”Ÿæˆå€ç‡ (Audio Generation Rate)
    if speed_stats:
        max_rate = np.max(speed_stats)
        min_rate = np.min(speed_stats)
        avg_rate = np.mean(speed_stats)
        print(f"  â€¢ ç”Ÿæˆå€ç‡ (Audio/Process Speed):")
        print(f"    (æ•¸å€¼ > 1.0 ä»£è¡¨ç”Ÿæˆé€Ÿåº¦æ¯”æ’­æ”¾é€Ÿåº¦å¿«)")
        print(f"      Avg : {avg_rate:.2f} x")
        print(f"      Max : {max_rate:.2f} x")
        print(f"      Min : {min_rate:.2f} x")
        
        # ä¼°ç®—æ•´é«” RTF
        total_audio_len = sum(e['duration'] for e in generation_events)
        overall_rtf = total_gen_time / total_audio_len if total_audio_len > 0 else 0
        print(f"  â€¢ æ•´é«”å¯¦æ™‚ç‡ (RTF): {overall_rtf:.3f} (è¶Šä½è¶Šå¥½)")
    else:
        print(f"  â€¢ ç”Ÿæˆå€ç‡: N/A")

    # ä¸¦è¡Œæ•ˆç‡
    overlap_count = 0
    playback_events = player.events
    if len(generation_events) > 1 and len(playback_events) > 0:
        for gen_event in generation_events[1:]:
            g_time = gen_event['timestamp']
            for play_event in playback_events:
                p_start = play_event['timestamp']
                p_end = p_start + (play_event['duration'] / args.speed)
                if p_start <= g_time <= p_end:
                    overlap_count += 1
                    break
        efficiency = (overlap_count / max(chunk_count - 1, 1)) * 100
        print(f"  â€¢ ä¸¦è¡Œæ•ˆç‡ (Parallel): {efficiency:.1f}% ({overlap_count}/{chunk_count-1} chunks overlapped)")
    
    print(f"{'='*80}\n")
    print("Done.")

if __name__ == "__main__":
    main()