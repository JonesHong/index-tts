import sys
import os
import time
import argparse
import threading
import queue
import gc
import warnings
import numpy as np
import sounddevice as sd
import soundfile as sf
import torch

# å¼•å…¥å¤–éƒ¨ä¾è³´
try:
    import pyrubberband as pyrb
    from opencc import OpenCC
except ImportError as e:
    print(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦å¥—ä»¶ {e.name}ã€‚è«‹ç¢ºä¿å·²å®‰è£ pyrubberband å’Œ opencc-python-reimplemented")
    sys.exit(1)

# ==================== 1. ç’°å¢ƒåˆå§‹åŒ– (ä½¿ç”¨ runtime_setup) ====================
import runtime_setup

# åˆå§‹åŒ–ä¸¦å–å¾—è·¯å¾‘
env_paths = runtime_setup.initialize(__file__)
INDEX_TTS_DIR = env_paths["INDEX_TTS_DIR"]

# è¨­å®š Python Path
sys.path.append(INDEX_TTS_DIR)
sys.path.append(os.path.join(INDEX_TTS_DIR, "indextts"))

# IndexTTS æ¨¡çµ„å°å…¥ (å¿…é ˆåœ¨ sys.path è¨­å®šå¾Œ)
from indextts.infer_v2 import IndexTTS2
from indextts.infer import IndexTTS
from indextts.infer_streaming_patch import add_streaming_to_indextts

# ==================== 2. å…¨åŸŸè¨­å®šèˆ‡å·¥å…·å‡½æ•¸ ====================

# ç¹ç°¡è½‰æ›
cc = OpenCC('t2s')

def convert_to_simplified(text):
    return cc.convert(text)

def get_timestamp(start_time):
    """ç²å–ç›¸å°æ™‚é–“æˆ³è¨˜ï¼ˆç§’ï¼‰"""
    return time.time() - start_time

def check_cuda():
    """æª¢æŸ¥ä¸¦æ‰“å° CUDA ç‹€æ…‹"""
    print(f"\n{'='*20} ç¡¬é«”ç‹€æ…‹ {'='*20}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬   : {torch.version.cuda}")
        print(f"GPU å‹è™Ÿ    : {torch.cuda.get_device_name(0)}")
        print(f"é¡¯å­˜ (ç¸½é‡) : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        torch.cuda.empty_cache()
    else:
        print("âš ï¸ CUDA ä¸å¯ç”¨, å°‡ä½¿ç”¨ CPU (é€Ÿåº¦æœƒå—å½±éŸ¿)")

# æ–‡å­—åˆ‡åˆ†é‚è¼¯
def split_text_smart(text, target_length=20, min_length=6):
    """å„ªåŒ–ç‰ˆåˆ‡åˆ†ï¼šå„ªå…ˆåœ¨æ¨™é»åˆ‡åˆ†"""
    punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€,.'
    max_length = int(target_length * 1.5)
    segments = []
    current_segment = ""
    
    for char in text:
        current_segment += char
        if len(current_segment) >= min_length and char in punctuation:
            clean_seg = current_segment.strip()
            if clean_seg: segments.append(clean_seg)
            current_segment = ""
        elif len(current_segment) >= max_length:
            clean_seg = current_segment.strip()
            if clean_seg: segments.append(clean_seg)
            current_segment = ""
            
    if current_segment.strip():
        segments.append(current_segment.strip())
        
    # äºŒæ¬¡æ¸…ç†
    return [seg.lstrip(punctuation).strip() for seg in segments if seg.lstrip(punctuation).strip()]

# ==================== 3. æ’­æ”¾å™¨é‚è¼¯ (ç¨ç«‹åŸ·è¡Œç·’) ====================

class AudioPlayer(threading.Thread):
    def __init__(self, sample_rate, speed_factor=1.0):
        super().__init__(daemon=True)
        self.queue = queue.Queue()
        self.active = threading.Event()
        self.active.set()
        self.sample_rate = sample_rate
        self.speed_factor = speed_factor
        self.events = [] # è¨˜éŒ„æ’­æ”¾äº‹ä»¶
        self.start_ref_time = 0

    def set_start_time(self, start_time):
        self.start_ref_time = start_time

    def put_chunk(self, audio_data, duration, chunk_id):
        self.queue.put((audio_data, duration, chunk_id))

    def stop(self):
        self.queue.put(None) # çµæŸä¿¡è™Ÿ
        self.join()

    def run(self):
        chunk_idx = 0
        print(f"[Player] æ’­æ”¾åŸ·è¡Œç·’å•Ÿå‹• (æ¡æ¨£ç‡: {self.sample_rate}, å€é€Ÿ: {self.speed_factor})")
        
        while self.active.is_set():
            try:
                item = self.queue.get(timeout=0.5)
                if item is None: break # æ”¶åˆ°çµæŸä¿¡è™Ÿ

                audio_normalized, original_duration, chunk_id = item
                chunk_idx += 1
                
                # è®Šé€Ÿè™•ç† (pyrubberband) - æ”¾åœ¨é€™è£¡åšæ˜¯ç‚ºäº†ä¸é˜»å¡ç”ŸæˆåŸ·è¡Œç·’
                if abs(self.speed_factor - 1.0) > 0.01:
                    audio_play = pyrb.time_stretch(audio_normalized, self.sample_rate, self.speed_factor)
                else:
                    audio_play = audio_normalized

                # è¨˜éŒ„é–‹å§‹
                play_start = get_timestamp(self.start_ref_time)
                self.events.append({'event': 'play_start', 'chunk': chunk_id, 'timestamp': play_start, 'duration': original_duration})
                
                print(f"[ğŸ”Š Play] ç‰‡æ®µ {chunk_id} é–‹å§‹æ’­æ”¾ (åŸå§‹æ™‚é•· {original_duration:.2f}s)")
                
                # æ’­æ”¾ (é˜»å¡ç›´åˆ°æ’­å®Œ)
                sd.play(audio_play, samplerate=self.sample_rate)
                sd.wait()

                # è¨˜éŒ„çµæŸ
                self.queue.task_done()
                
            except queue.Empty:
                continue
        print("[Player] æ’­æ”¾åŸ·è¡Œç·’çµæŸ")

# ==================== 4. ä¸»ç¨‹å¼é‚è¼¯ ====================

def main():
    # --- 0. å±è”½å¹²æ“¾è¨Šæ¯ ---
    warnings.filterwarnings("ignore", category=FutureWarning) 
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
    ref_audio_dir = os.path.join(INDEX_TTS_DIR, "examples")
    ref_audio_dist = {
        "voice_03.wav":  os.path.join(ref_audio_dir, "voice_03.wav"),
        "voice_06.wav":  os.path.join(ref_audio_dir, "voice_06.wav"),
        "voice_07.wav":  os.path.join(ref_audio_dir, "voice_07.wav"),
        "voice_11.wav":  os.path.join(ref_audio_dir, "voice_11.wav"),
        "é˜¿ç’‹.wav":  os.path.join(ref_audio_dir, "é˜¿ç’‹.wav"),
        "GY.wav":  os.path.join(ref_audio_dir, "GY.wav"),
        "DIDI.wav":  os.path.join(ref_audio_dir, "DIDI.wav"),
        "JADE.wav":  os.path.join(ref_audio_dir, "JADE.wav"),
        "Joneshong.wav":  os.path.join(ref_audio_dir, "Joneshong.wav"),
        "Sean.wav":  os.path.join(ref_audio_dir, "Sean.wav"),
    }
    # --- åƒæ•¸è§£æ ---
    parser = argparse.ArgumentParser(description="Index-TTS Streaming Test")
    parser.add_argument("--version", type=str, default="v2", choices=["v1", "v2"], help="æ¨¡å‹ç‰ˆæœ¬")
    parser.add_argument("--method", type=str, default="token", choices=["token", "word"], help="åˆ‡åˆ†æ–¹æ³• (v2 Only)")
    parser.add_argument("--model_dir", type=str, default=None, help="æ¨¡å‹è³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--ref_audio", type=str, default=ref_audio_dist["voice_07.wav"], help="åƒè€ƒéŸ³é »è·¯å¾‘")
    parser.add_argument("--speed", type=float, default=1.3, help="æ’­æ”¾èªé€Ÿ")
    parser.add_argument("--text", type=str, default=None, help="æ¸¬è©¦æ–‡æœ¬")
    parser.add_argument("--steps", type=int, default=25, help="æ“´æ•£æ¨¡å‹æ­¥æ•¸ (åƒ…åƒè€ƒ)")
    
    args = parser.parse_args()

    # --- æ–‡æœ¬è™•ç† ---
    default_text = (
        "ä¸€åè‡ªç¨±å°å¤§å¤§æ°£ç³»å­¸ç”Ÿçš„ç¶²å‹åœ¨è‡‰æ›¸ã€Œé»‘ç‰¹å¸å¤§ã€ç«‹ä¸‹è±ªè¨€ï¼Œé æ¸¬å°åŒ—å¸‚åä¸€æœˆåäºŒæ—¥è‡³åå››æ—¥è‡³å°‘æœƒå› é³³å‡°é¢±é¢¨æ”¾å‡å…©å¤©ï¼Œ"
        "ä¸¦ä»¥ä¸‰ç™¾ä»½é›æ’çå¥¶ä½œç‚ºå¤±æº–æ™‚çš„ç¥­å“ã€‚çµæœé€™ä½å­¸ç”Ÿè¢«ç¾å¯¦ç‹ ç‹ æ‰“è‡‰ï¼Œæœ€çµ‚ä»–ä¹Ÿå¯¦ç¾æ‰¿è«¾ï¼Œ"
        "å®£å¸ƒåä¸€æœˆåå…­æ—¥ä¸­åˆå°±åœ¨å°å¤§æ ¡åœ’å…§ç™¼é€é›æ’ã€çå¥¶ï¼Œè®“ç¶²å‹æ¶ç•™è¨€å¡ä½ã€‚"
    )
    target_text = args.text if args.text else default_text
    text_simplified = convert_to_simplified(target_text)

    # --- é¡¯ç¤ºé…ç½® ---
    print(f"\n{'='*20} æ¸¬è©¦é…ç½® {'='*20}")
    print(f"ç‰ˆæœ¬: {args.version}")
    print(f"æ–¹æ³•: {args.method}")
    print(f"èªé€Ÿ: {args.speed}")
    print(f"åŸæ–‡: {target_text[:30]}...")
    
    check_cuda()

    # --- è¼‰å…¥æ¨¡å‹ ---
    print("\n=== è¼‰å…¥æ¨¡å‹ä¸­... ===")
    start_load = time.time()
    
    tts_model = None
    sampling_rate = 22050 

    if args.version == "v2":
        model_dir = args.model_dir or os.path.join(INDEX_TTS_DIR, "checkpoints_v2")
        config_path = os.path.join(model_dir, "config.yaml")
        sampling_rate = 22050
        
        tts_model = IndexTTS2(
            cfg_path=config_path,
            model_dir=model_dir,
            use_fp16=True,
            use_cuda_kernel=False,
            use_deepspeed=False,
            use_accel=False,
            use_torch_compile=False
        )
    else: # v1
        model_dir = args.model_dir or os.path.join(INDEX_TTS_DIR, "checkpoints_v1.5")
        config_path = os.path.join(model_dir, "config.yaml")
        sampling_rate = 24000
        
        tts_model = IndexTTS(
            model_dir=model_dir,
            cfg_path=config_path,
            use_fp16=True,
            use_cuda_kernel=False
        )
        tts_model = add_streaming_to_indextts(tts_model)

    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ (è€—æ™‚: {time.time() - start_load:.2f}s)")

    # --- æº–å‚™æ’­æ”¾å™¨ ---
    player = AudioPlayer(sample_rate=sampling_rate, speed_factor=args.speed)
    player.start()

    # --- æº–å‚™ç”Ÿæˆ ---
    processing_queue = [] 
    if args.version == "v2" and args.method == "token":
        processing_queue.append((text_simplified, "full_text"))
    else:
        segments = split_text_smart(text_simplified)
        print(f"ğŸ“ æ‰‹å‹•åˆ‡åˆ†: å…± {len(segments)} æ®µ")
        for i, seg in enumerate(segments):
            processing_queue.append((seg, f"segment_{i+1}"))

    # --- é–‹å§‹ç”Ÿæˆè¿´åœˆ ---
    global_start_time = time.time()
    player.set_start_time(global_start_time)
    
    chunk_count = 0
    first_chunk_time = None
    generation_events = []
    
    # [çµ±è¨ˆä¿®æ­£] æ”¹ç”¨ Audio/Sec (ç”Ÿæˆå€ç‡)
    speed_stats = [] 
    
    # [æ–°å¢] è©³ç´°æ™‚é–“åˆ†è§£çµ±è¨ˆ
    time_breakdown = {
        'audio_processing': [],     # éŸ³è¨Šè™•ç†æ™‚é–“ï¼ˆè½‰æ›ã€è§£ç¢¼ï¼‰
        'gpu_transfer': [],         # GPU -> CPU å‚³è¼¸æ™‚é–“
        'waiting': [],              # ç­‰å¾…æ™‚é–“ï¼ˆç”Ÿæˆå™¨ä¹‹é–“çš„é–“éš”ï¼‰
        'total_latency': []         # ç¸½å»¶é²
    }

    print(f"\n[ğŸš€ Start] é–‹å§‹ä¸²æµç”Ÿæˆ...")

    for text_input, label in processing_queue:
        print(f"[ğŸ¬ Gen] æ­£åœ¨è™•ç†: {label} ({len(text_input)}å­—)")

        audio_generator = None
        
        if args.version == "v2":
            kwargs = {
                "spk_audio_prompt": args.ref_audio,
                "text": text_input,
                "output_path": None,
                "stream_return": True,
                "interval_silence": 250,
                "verbose": False,
                "use_emo_text": False,
                "emo_vector": None
            }
            if args.method == "token":
                kwargs["max_text_tokens_per_segment"] = 60
            audio_generator = tts_model.infer(**kwargs)
        else:
            audio_generator = tts_model.infer_stream(args.ref_audio, text_input, verbose=False)

        # è¨ˆæ™‚åˆå§‹åŒ–
        t_last_chunk_finish = time.time()
        t_iterator_start = None

        for audio_chunk in audio_generator:
            # [æ™‚é–“é» 1] æ”¶åˆ° chunk çš„æ™‚é–“ï¼ˆæ­¤æ™‚æ¨¡å‹å·²å®Œæˆæ¨ç†ï¼‰
            t_chunk_received = time.time()
            t_now_rel = get_timestamp(global_start_time)
            
            # è¨ˆç®—ç­‰å¾…æ™‚é–“ï¼ˆå¾ä¸Šä¸€å€‹ chunk å®Œæˆåˆ°é€™å€‹ chunk é–‹å§‹ï¼‰
            if t_iterator_start is not None:
                waiting_time = t_iterator_start - t_last_chunk_finish
                time_breakdown['waiting'].append(max(0, waiting_time))
            
            chunk_count += 1

            # [æ™‚é–“é» 2] Tensor é è™•ç†ï¼ˆåœ¨ CPU/GPU ä¸Šï¼‰
            t_preprocess_start = time.time()
            if isinstance(audio_chunk, list):
                audio_chunk = torch.cat(audio_chunk, dim=-1) if len(audio_chunk) > 0 else torch.zeros(1)
            t_preprocess_end = time.time()
            preprocess_time = t_preprocess_end - t_preprocess_start
            
            # [æ™‚é–“é» 3] GPU -> CPU å‚³è¼¸
            t_before_cpu = time.time()
            audio_np = audio_chunk.cpu().numpy().squeeze()
            t_after_cpu = time.time()
            gpu_transfer_time = t_after_cpu - t_before_cpu
            
            # [æ™‚é–“é» 4] CPU ç«¯éŸ³è¨Šæ ¼å¼è½‰æ›
            t_format_start = time.time()
            audio_normalized = audio_np.astype(np.float32) / 32767.0
            duration = audio_np.shape[-1] / sampling_rate
            t_format_end = time.time()
            format_time = t_format_end - t_format_start
            
            # è¨ˆç®—å„éšæ®µæ™‚é–“
            audio_processing_time = preprocess_time + format_time  # ä¸åŒ…å« GPU å‚³è¼¸
            chunk_latency = t_format_end - t_last_chunk_finish
            
            # å„²å­˜æ™‚é–“åˆ†è§£æ•¸æ“šï¼ˆæ’é™¤å¤ªçŸ­çš„ç‰‡æ®µï¼‰
            if duration > 0.05:
                time_breakdown['total_latency'].append(chunk_latency)
                time_breakdown['gpu_transfer'].append(gpu_transfer_time)
                time_breakdown['audio_processing'].append(audio_processing_time)
            
            t_last_chunk_finish = t_format_end
            
            # è¨ˆç®—ç”Ÿæˆå€ç‡ (Audio Sec / Process Sec)
            # æ•¸å€¼ > 1 ä»£è¡¨ç”Ÿæˆæ¯”æ’­æ”¾å¿« (ç†æƒ³æƒ…æ³)
            if chunk_latency > 0.01:
                gen_rate = duration / chunk_latency
                speed_stats.append(gen_rate)
            else:
                gen_rate = 0
            
            if duration < 0.05: 
                t_iterator_start = time.time()
                continue 

            if first_chunk_time is None:
                first_chunk_time = t_now_rel
                print(f"[âš¡ First Token] é¦–å€‹éŸ³è¨Šå·²ç”Ÿæˆ: {first_chunk_time:.2f}s")

            generation_events.append({
                'event': 'generate',
                'chunk': chunk_count,
                'timestamp': t_now_rel,
                'duration': duration,
                'latency': chunk_latency,
                'gpu_transfer': gpu_transfer_time,
                'audio_processing': audio_processing_time
            })

            # é€™è£¡é¡¯ç¤º Gen Rate (å€ç‡) å’Œè©³ç´°æ™‚é–“
            if duration > 0.1:
                print(f"  -> [Queue] ç‰‡æ®µ {chunk_count} (éŸ³é•· {duration:.2f}s, è€—æ™‚ {chunk_latency:.2f}s, å€ç‡ {gen_rate:.2f}x)")
                print(f"             [ç´°ç¯€] GPUå‚³è¼¸ {gpu_transfer_time*1000:.1f}ms | éŸ³è¨Šè™•ç† {audio_processing_time*1000:.1f}ms")
                player.put_chunk(audio_normalized, duration, chunk_count)
            else:
                print(f"  -> [Queue] ç‰‡æ®µ {chunk_count} (éŸ³é•· {duration:.2f}s, è€—æ™‚ {chunk_latency:.2f}s, å€ç‡ {gen_rate:.2f}x)")
                player.put_chunk(audio_normalized, duration, chunk_count)
            
            # è¨˜éŒ„ä¸‹ä¸€æ¬¡è¿­ä»£é–‹å§‹æ™‚é–“
            t_iterator_start = time.time()

    total_gen_time = get_timestamp(global_start_time)
    print(f"\n[ğŸ Finish] æ‰€æœ‰ç”Ÿæˆä»»å‹™å®Œæˆ (ç¸½è€—æ™‚: {total_gen_time:.2f}s)")
    
    player.stop()

    # ==================== 5. ç¶œåˆçµ±è¨ˆå ±å‘Š ====================
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç¶œåˆçµ±è¨ˆå ±å‘Š")
    print(f"{'='*80}")

    # --- A. åƒæ•¸é…ç½® ---
    print(f"ğŸ”§ åŸ·è¡Œåƒæ•¸ (Arguments):")
    for k, v in vars(args).items():
        print(f"  â€¢ {k:<12} : {v}")
    
    # --- B. åƒè€ƒéŸ³è¨Šåˆ†æ ---
    print(f"\nğŸµ åƒè€ƒéŸ³è¨Šè³‡è¨Š (Ref Audio):")
    if os.path.exists(args.ref_audio):
        try:
            file_size_bytes = os.path.getsize(args.ref_audio)
            file_size_mb = file_size_bytes / (1024 * 1024)
            sf_info = sf.info(args.ref_audio)
            duration = sf_info.duration
            samplerate = sf_info.samplerate
            subtype = sf_info.subtype
            bitrate_kbps = (file_size_bytes * 8) / duration / 1000 if duration > 0 else 0
            
            print(f"  â€¢ æª”å (File)    : {os.path.basename(args.ref_audio)}")
            print(f"  â€¢ å¤§å° (Size)    : {file_size_mb:.2f} MB")
            print(f"  â€¢ ç§’æ•¸ (Length)  : {duration:.2f} s")
            print(f"  â€¢ æ ¼å¼ (Format)  : {sf_info.format} ({subtype})")
            print(f"  â€¢ æ¡æ¨£ (Rate)    : {samplerate} Hz")
            print(f"  â€¢ ç¢¼ç‡ (Bitrate) : {bitrate_kbps:.0f} kbps")
        except Exception as e:
            print(f"  âš ï¸ ç„¡æ³•è®€å–éŸ³è¨Šè³‡è¨Š: {e}")
    else:
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {args.ref_audio}")

    # --- C. æ•ˆèƒ½çµ±è¨ˆ ---
    print(f"{'-'*40}")
    print(f"ğŸš€ æ•ˆèƒ½æŒ‡æ¨™ (Performance):")
    print(f"  â€¢ é¦–æ¬¡éŸ¿æ‡‰ (TTFT): {first_chunk_time if first_chunk_time else 'N/A'}")
    print(f"  â€¢ ç¸½è€—æ™‚   (Total): {total_gen_time:.2f} s")
    print(f"  â€¢ ç¸½ç‰‡æ®µæ•¸ (Chunks): {chunk_count}")

    # [ä¿®æ­£] é¡¯ç¤ºç”Ÿæˆå€ç‡ (Audio Generation Rate)
    if speed_stats:
        max_rate = np.max(speed_stats)
        min_rate = np.min(speed_stats)
        avg_rate = np.mean(speed_stats)
        print(f"  â€¢ ç”Ÿæˆå€ç‡ (Audio/Process Speed):")
        print(f"    (æ•¸å€¼ > 1.0 ä»£è¡¨ç”Ÿæˆé€Ÿåº¦æ¯”æ’­æ”¾é€Ÿåº¦å¿«)")
        print(f"      Avg : {avg_rate:.2f} x")
        print(f"      Max : {max_rate:.2f} x")
        print(f"      Min : {min_rate:.2f} x")
        
        # ä¼°ç®—æ•´é«” RTF
        total_audio_len = sum(e['duration'] for e in generation_events)
        overall_rtf = total_gen_time / total_audio_len if total_audio_len > 0 else 0
        print(f"  â€¢ æ•´é«”å¯¦æ™‚ç‡ (RTF): {overall_rtf:.3f} (è¶Šä½è¶Šå¥½)")
    else:
        print(f"  â€¢ ç”Ÿæˆå€ç‡: N/A")

    # [æ–°å¢] æ™‚é–“åˆ†è§£çµ±è¨ˆ
    if time_breakdown['total_latency']:
        print(f" â±ï¸  æ™‚é–“åˆ†è§£ (Time Breakdown):")
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_total = np.mean(time_breakdown['total_latency'])
        avg_gpu = np.mean(time_breakdown['gpu_transfer'])
        avg_audio_proc = np.mean(time_breakdown['audio_processing'])
        
        # è¨ˆç®—æ¨¡å‹å…§éƒ¨è™•ç†æ™‚é–“ï¼ˆç¸½å»¶é² - GPUå‚³è¼¸ - éŸ³è¨Šè™•ç†ï¼‰
        # é€™åŒ…æ‹¬ï¼šæ–‡æœ¬ç·¨ç¢¼ã€æ¨¡å‹æ¨ç†ã€melç”Ÿæˆã€vocoderè§£ç¢¼ç­‰
        model_inference_times = []
        for i in range(len(time_breakdown['total_latency'])):
            model_time = (time_breakdown['total_latency'][i] - 
                         time_breakdown['gpu_transfer'][i] - 
                         time_breakdown['audio_processing'][i])
            model_inference_times.append(max(0, model_time))  # é¿å…è² æ•¸
        avg_model = np.mean(model_inference_times)
        
        # è¨ˆç®—ä½”æ¯”
        model_ratio = (avg_model / avg_total * 100) if avg_total > 0 else 0
        audio_ratio = (avg_audio_proc / avg_total * 100) if avg_total > 0 else 0
        gpu_ratio = (avg_gpu / avg_total * 100) if avg_total > 0 else 0
        
        print(f"    â€¢ å–®å€‹ Chunk å¹³å‡å»¶é²: {avg_total:.3f}s")
        print(f"      â”œâ”€ æ¨¡å‹å…§éƒ¨è™•ç†:   {avg_model:.3f}s ({model_ratio:.1f}%)")
        print(f"      â”‚  (æ–‡æœ¬ç·¨ç¢¼ + æ¨¡å‹æ¨ç† + melç”Ÿæˆ + vocoder)")
        print(f"      â”œâ”€ GPUâ†’CPU å‚³è¼¸:   {avg_gpu:.3f}s ({gpu_ratio:.1f}%)")
        print(f"      â””â”€ éŸ³è¨Šå¾Œè™•ç†:     {avg_audio_proc:.3f}s ({audio_ratio:.1f}%)")
        print(f"         (Tensorè™•ç† + æ ¼å¼è½‰æ›)")
        
        # ç“¶é ¸åˆ†æ
        print(f" ğŸ’¡ ç“¶é ¸åˆ†æ:")
        if model_ratio > 80:
            print(f"      ä¸»è¦ç“¶é ¸åœ¨ã€Œæ¨¡å‹å…§éƒ¨è™•ç†ã€({model_ratio:.1f}%)")
            print(f"      å»ºè­°: å„ªåŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦ã€ä½¿ç”¨æ›´å¿«çš„ GPUã€é™ä½ steps")
        elif audio_ratio > 50:
            print(f"      ä¸»è¦ç“¶é ¸åœ¨ã€ŒéŸ³è¨Šå¾Œè™•ç†ã€({audio_ratio:.1f}%)")
            print(f"      å»ºè­°: å„ªåŒ–éŸ³è¨Šè§£ç¢¼ã€æ¸›å°‘ CPU è™•ç†ã€ä½¿ç”¨æ‰¹æ¬¡è™•ç†")
        else:
            print(f"      å„éšæ®µè€—æ™‚ç›¸å°å‡è¡¡")
            print(f"      å»ºè­°: å…¨é¢å„ªåŒ–ï¼Œç‰¹åˆ¥é—œæ³¨ä½”æ¯”æœ€é«˜çš„éƒ¨åˆ†")
        
        # GPU å‚³è¼¸è­¦å‘Š
        if gpu_ratio > 10:
            print(f"      âš ï¸  GPU å‚³è¼¸ä½”æ¯”è¼ƒé«˜ ({gpu_ratio:.1f}%)ï¼Œè€ƒæ…®æ¸›å°‘å‚³è¼¸æ¬¡æ•¸")
        
        # é¡¯ç¤ºå¯¦éš›æ•¸å€¼
        print(f" ğŸ“ˆ è©³ç´°æ•¸å€¼:")
        print(f"      æ¨¡å‹å…§éƒ¨: {avg_model*1000:.1f}ms ({model_ratio:.1f}%)")
        print(f"      GPUå‚³è¼¸:  {avg_gpu*1000:.1f}ms ({gpu_ratio:.1f}%)")
        print(f"      éŸ³è¨Šè™•ç†: {avg_audio_proc*1000:.1f}ms ({audio_ratio:.1f}%)")

    # ä¸¦è¡Œæ•ˆç‡
    overlap_count = 0
    playback_events = player.events
    if len(generation_events) > 1 and len(playback_events) > 0:
        for gen_event in generation_events[1:]:
            g_time = gen_event['timestamp']
            for play_event in playback_events:
                p_start = play_event['timestamp']
                p_end = p_start + (play_event['duration'] / args.speed)
                if p_start <= g_time <= p_end:
                    overlap_count += 1
                    break
        efficiency = (overlap_count / max(chunk_count - 1, 1)) * 100
        print(f" â€¢ ä¸¦è¡Œæ•ˆç‡ (Parallel): {efficiency:.1f}% ({overlap_count}/{chunk_count-1} chunks overlapped)")
    
    print(f"{'='*80}\n")
    print("Done.")

if __name__ == "__main__":
    main()